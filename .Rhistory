fontface = "bold") +
scale_x_log10(
breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
labels = c("0.001", "0.01", "0.1", "1", "10", "100", "1,000")
) +
labs(
title = "NYPD-only Service Requests with Positive Durations",
subtitle = sprintf("n = %s, Median = %.2f days, Mean = %.2f days",
format(nrow(nypd_data), big.mark = ","),
nypd_median,
nypd_mean),
x = "Days (log scale)",
y = "Count"
) +
david_theme()
print(p1)
Sys.sleep(3)
# Calculate other agencies mean and median
other_mean <- mean(other_data$duration_days)
other_median <- median(other_data$duration_days)
# Other agencies histogram with mean and median lines
p2 <- ggplot(other_data, aes(x = duration_days)) +
geom_histogram(bins = 150, fill = "#009E73", alpha = 0.85, color = "white",
linewidth = 0.05) +
geom_vline(xintercept = other_mean, color = "#999999", linewidth = 1.5,
linetype = "dashed") +
annotate("text", x = other_mean, y = Inf,
label = sprintf("Mean = %.2f days", other_mean),
vjust = 3, hjust = -0.1, color = "#999999", size = 4,
fontface = "bold") +
geom_vline(xintercept = other_median, color = "#D55E00", linewidth = 1.5,
linetype = "dotted") +
annotate("text", x = other_median, y = Inf,
label = sprintf("Median = %.2f days", other_median),
vjust = 3.0, hjust = 1.15, color = "#D55E00", size = 4,
fontface = "bold") +
scale_x_log10(
breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
labels = c("0.001", "0.01", "0.1", "1", "10", "100", "1,000")
) +
labs(
title = "Non-NYPD Service Requests with Positive Durations",
subtitle = sprintf("n = %s, Median = %.2f days, Mean = %.2f days",
format(nrow(other_data), big.mark = ","),
other_median,
other_mean),
x = "Days (log scale)",
y = "Count"
) +
david_theme()
print(p2)
Sys.sleep(3)
# Save individual plots
ggsave(file.path(chart_dir, "nypd_only_positive_durations.pdf"), p1,
width = 13, height = 8.5, units = "in")
ggsave(file.path(chart_dir, "others_only_positive_durations.pdf"), p2,
width = 13, height = 8.5, units = "in")
# Combine data with agency group label
combined_data <- rbind(
nypd_data[, .(duration_days, group = "NYPD")],
other_data[, .(duration_days, group = "Other Agencies")]
)
p_combined <- ggplot(combined_data, aes(x = duration_days, fill = group)) +
geom_histogram(bins = 150, alpha = 0.7, position = "identity",
color = "white", linewidth = 0.1) +
scale_fill_manual(values = c("NYPD" = "#0072B2", "Other Agencies" = "#009E73")) +
scale_x_log10(
breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
labels = c("0.001", "0.01", "0.1", "1", "10", "100", "1,000")
) +
labs(
title = "Bimodal Distribution: NYPD vs Other Agencies",
subtitle = sprintf("NYPD n = %s, Other n = %s",
format(nrow(nypd_data), big.mark = ","),
format(nrow(other_data), big.mark = ",")),
x = "Days (log scale)",
y = "Count",
fill = "Agency Group"
) +
david_theme() +
theme(
legend.position = "inside",
legend.position.inside = c(0.15, 0.85),  # Upper left (x, y from 0-1)
legend.background = element_rect(fill = "white", color = "#999999")
)
print(p_combined)
Sys.sleep(3)
ggsave(file.path(chart_dir, "nypd_vs_others_combined.pdf"), p_combined,
width = 13, height = 8.5, units = "in")
# p_density <- ggplot(combined_data, aes(x = duration_days, color = group, fill = group)) +
#   geom_density(alpha = 0.3, linewidth = 1.5) +
#   scale_color_manual(values = c("NYPD" = "#0072B2", "Other Agencies" = "#009E73")) +
#   scale_fill_manual(values = c("NYPD" = "#0072B2", "Other Agencies" = "#009E73")) +
#   scale_x_log10(
#     breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
#     labels = c("0.001", "0.01", "0.1", "1", "10", "100", "1,000")
#   ) +
#   labs(
#     title = "Bimodal Distribution: NYPD vs Other Agencies",
#     subtitle = sprintf("NYPD n = %s, Other n = %s",
#                        format(nrow(nypd_data), big.mark = ","),
#                        format(nrow(other_data), big.mark = ",")),
#     x = "Days (log scale)",
#     y = "Density",
#     color = "Agency Group",
#     fill = "Agency Group"
#   ) +
#   david_theme()
#
# print(p_density)
# p_stacked <- ggplot(combined_data, aes(x = duration_days, fill = group)) +
#   geom_histogram(bins = 150, alpha = 0.85, color = "white", linewidth = 0.1) +
#   scale_fill_manual(values = c("NYPD" = "#0072B2", "Other Agencies" = "#009E73")) +
#   scale_x_log10(
#     breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
#     labels = c("0.001", "0.01", "0.1", "1", "10", "100", "1,000")
#   ) +
#   labs(
#     title = "Bimodal Distribution: NYPD vs Other Agencies",
#     subtitle = "Stacked to show relative contributions",
#     x = "Days (log scale)",
#     y = "Count",
#     fill = "Agency Group"
#   ) +
#   david_theme()
#
# print(p_stacked)
# Set histogram display limits for readability
upper_limit <- 30*3    # Maximum days to display (90 days)
lower_limit <- 2       # Minimum days to display
# Create bounded dataset for plotting
limited_positive_data <- positive_data[
duration_days >= lower_limit & duration_days <= upper_limit
]
# Generate summary statistics
summary(positive_data$duration_day)
# Count records within plotting bounds
n_plotted <- nrow(limited_positive_data)
plot_histogram(
DT         = positive_data,
value_col  = "duration_days",
title      = sprintf("Positive Duration Distribution (<= %s days)",
upper_limit),
x_label    = "Duration (days)",
add_labels = TRUE,
chart_dir  = chart_dir,
filename   = "positive_duration_histogram.pdf",
bins       = 300,
alpha      = 0.8,
outlier_percentile = 1.0,
add_stats  = TRUE,
width      = 13,
height     = 8.5,
xlim       = c(0, upper_limit)   # <-- NEW ARGUMENT
)
# ==============================================================================
# SECTION 2: NEGATIVE DURATION ANALYSIS
# ==============================================================================
# Analyze service requests with negative durations (data quality issues)
# Negative durations indicate closed_date occurs before created_date
cat("\n=== ANALYZING NEGATIVE DURATIONS ===\n")
# Filter to negative duration records
negative_data <- d311[duration_days < 0 & !is.na(duration_days)]
# Set histogram limits for negative values
upper_limit_neg <- 0      # Less negative (closer to zero)
lower_limit_neg <- -365   # More negative (further from zero)
# Create bounded dataset for plotting
limited_negative_data <- negative_data[
duration_days >= lower_limit_neg & duration_days <= upper_limit_neg
]
# Generate summary statistics
summary(negative_data$duration_day)
# Count records within plotting bounds
n_plotted_neg <- nrow(limited_negative_data)
plot_histogram(
DT         = limited_negative_data,
value_col  = "duration_days",
# Titles and labels
title      = sprintf("Negative Duration Distribution (%d to %d days)",
lower_limit_neg, upper_limit_neg),
x_label    = "Duration (days)",
chart_dir  = chart_dir,
filename   = "negative_duration_histogram.pdf",
add_labels = FALSE,        # <-- ADD THIS
# Visual appearance
bins       = 100,
fill_color = "#D55E00",
alpha      = 0.7,
# Bounds
xlim       = c(lower_limit_neg, upper_limit_neg),
outlier_percentile = 1.0,   # donâ€™t trim â€” youâ€™re bounding manually
# Stats & output
add_stats  = TRUE,
width      = 13,
height     = 8.5
)
plot_result <- plot_boxplot(
DT        = limited_negative_data,
value_col = duration_days,
by_col    = agency,
chart_dir = chart_dir,
filename  = "negative_duration_SR_boxplot.pdf",
title     = " Negative Duration (days) by agency",
top_n     = 30,
y_axis_tick_size = 10,
order_by  = "count",
flip      = TRUE,
x_scale_type = "pseudo_log",
x_limits = c(lower_limit, upper_limit),
min_count = 5,  # FIXED: was min_agency_obs (which defaults to 1)
jitter_size = 1.3,
jitter_alpha = 0.55,
outlier_size = 1.4,
count_label_hjust = label_hjust,
show_count_labels = show_count_labels
)
create_violin_chart(
dataset = limited_negative_data,
x_axis_field = "duration_days",
chart_directory = chart_dir,
chart_file_name = "negative_duration_SR_violin.pdf",
chart_title = "Distribution of Negative Duration Days"
)
# ==============================================================================
# SECTION 3: SHORT DURATION ANALYSIS & THRESHOLD DETERMINATION
# ==============================================================================
# Analyze very short durations to identify suspicious patterns
# Determines statistical threshold for flagging anomalously short durations
cat("\n=== ANALYZING SHORT DURATIONS & SETTING THRESHOLDS ===\n")
# Run comprehensive skewed duration analysis
skewed_result <- analyze_skewed_durations(
DT = d311,
duration_col = "duration_days",
minimum_cutoff_sec = 2,
upper_cutoff_sec = 60*60*24*2L,  # 2 days in seconds
print_summary = TRUE,
create_plots = TRUE,
chart_dir = chart_dir
)
# Extract the LogNormal 3-standard-deviation threshold
threshold <- skewed_result$thresholds$log_3sd_lower
threshold_numeric <- round(as.numeric(threshold), 0)
# Create detailed histogram with threshold visualization
plot_duration_histogram(
DT = d311,
duration_col = "duration_days",
bin_width = 1,
x_label_skip = 2,         # Show every 2nd x-axis label
x_axis_angle = 45,        # Rotate labels for readability
max_value = 90,           # Focus on first 90 seconds
min_value = 2L,
threshold_numeric = threshold_numeric,
chart_dir = chart_dir
)
# Display threshold value
cat("LogNormal_3SD threshold:", threshold_numeric, "seconds\n")
# ==============================================================================
# SECTION 4: COMPREHENSIVE DURATION CATEGORY ANALYSIS
# ==============================================================================
# Systematic analysis of all duration categories:
# - Negative (small, large, extreme)
# - Zero durations
# - Near-zero (seconds-level)
# - One-second durations
# - Positive (small, large, extreme)
cat("\n=== COMPREHENSIVE DURATION CATEGORY ANALYSIS ===\n")
duraton_analysis <- analyze_duration_QA(d311,
chart_dir = chart_dir)
source("~/datacleaningproject/journal_of_data_science/nyc_311_data_cleaning/code/functions/analyze_duration_category.R")
# ==============================================================================
# SECTION 2: NEGATIVE DURATION ANALYSIS
# ==============================================================================
# Analyze service requests with negative durations (data quality issues)
# Negative durations indicate closed_date occurs before created_date
cat("\n=== ANALYZING NEGATIVE DURATIONS ===\n")
# Filter to negative duration records
negative_data <- d311[duration_days < 0 & !is.na(duration_days)]
# Set histogram limits for negative values
upper_limit_neg <- 0      # Less negative (closer to zero)
lower_limit_neg <- -365   # More negative (further from zero)
# Create bounded dataset for plotting
limited_negative_data <- negative_data[
duration_days >= lower_limit_neg & duration_days <= upper_limit_neg
]
# Generate summary statistics
summary(negative_data$duration_day)
# Count records within plotting bounds
n_plotted_neg <- nrow(limited_negative_data)
plot_histogram(
DT         = limited_negative_data,
value_col  = "duration_days",
# Titles and labels
title      = sprintf("Negative Duration Distribution (%d to %d days)",
lower_limit_neg, upper_limit_neg),
x_label    = "Duration (days)",
chart_dir  = chart_dir,
filename   = "negative_duration_histogram.pdf",
add_labels = FALSE,        # <-- ADD THIS
# Visual appearance
bins       = 100,
fill_color = "#D55E00",
alpha      = 0.7,
# Bounds
xlim       = c(lower_limit_neg, upper_limit_neg),
outlier_percentile = 1.0,   # donâ€™t trim â€” youâ€™re bounding manually
# Stats & output
add_stats  = TRUE,
width      = 13,
height     = 8.5
)
plot_result <- plot_boxplot(
DT        = limited_negative_data,
value_col = duration_days,
by_col    = agency,
chart_dir = chart_dir,
filename  = "negative_duration_SR_boxplot.pdf",
title     = " Negative Duration (days) by agency",
top_n     = 30,
y_axis_tick_size = 10,
order_by  = "count",
flip      = TRUE,
x_scale_type = "pseudo_log",
x_limits = c(lower_limit, upper_limit),
min_count = 5,  # FIXED: was min_agency_obs (which defaults to 1)
jitter_size = 1.3,
jitter_alpha = 0.55,
outlier_size = 1.4,
count_label_hjust = label_hjust,
show_count_labels = show_count_labels
)
create_violin_chart(
dataset = limited_negative_data,
x_axis_field = "duration_days",
chart_directory = chart_dir,
chart_file_name = "negative_duration_SR_violin.pdf",
chart_title = "Distribution of Negative Duration Days"
)
# ==============================================================================
# SECTION 3: SHORT DURATION ANALYSIS & THRESHOLD DETERMINATION
# ==============================================================================
# Analyze very short durations to identify suspicious patterns
# Determines statistical threshold for flagging anomalously short durations
cat("\n=== ANALYZING SHORT DURATIONS & SETTING THRESHOLDS ===\n")
# Run comprehensive skewed duration analysis
skewed_result <- analyze_skewed_durations(
DT = d311,
duration_col = "duration_days",
minimum_cutoff_sec = 2,
upper_cutoff_sec = 60*60*24*2L,  # 2 days in seconds
print_summary = TRUE,
create_plots = TRUE,
chart_dir = chart_dir
)
# Extract the LogNormal 3-standard-deviation threshold
threshold <- skewed_result$thresholds$log_3sd_lower
threshold_numeric <- round(as.numeric(threshold), 0)
# Create detailed histogram with threshold visualization
plot_duration_histogram(
DT = d311,
duration_col = "duration_days",
bin_width = 1,
x_label_skip = 2,         # Show every 2nd x-axis label
x_axis_angle = 45,        # Rotate labels for readability
max_value = 90,           # Focus on first 90 seconds
min_value = 2L,
threshold_numeric = threshold_numeric,
chart_dir = chart_dir
)
# Display threshold value
cat("LogNormal_3SD threshold:", threshold_numeric, "seconds\n")
# ==============================================================================
# SECTION 4: COMPREHENSIVE DURATION CATEGORY ANALYSIS
# ==============================================================================
# Systematic analysis of all duration categories:
# - Negative (small, large, extreme)
# - Zero durations
# - Near-zero (seconds-level)
# - One-second durations
# - Positive (small, large, extreme)
cat("\n=== COMPREHENSIVE DURATION CATEGORY ANALYSIS ===\n")
duraton_analysis <- analyze_duration_QA(d311,
chart_dir = chart_dir)
cat("\n=== RESPONSE TIMES BY COMPLAINT CATEGORY ANALYSIS ===\n")
complaint_stats <- summarize_complaint_response(
d311,
print_top = 300,
min_records = 50)
# ==============================================================================
# END OF DURATION ANALYSIS
# ==============================================================================
################################################################################
# Close program
close_program(
program_start = timing$program_start,
enable_sink = enable_sink,
verbose = TRUE
)
################################################################################
################################################################################
source("~/datacleaningproject/journal_of_data_science/nyc_311_data_cleaning/code/jds_datacleansings.R")
message("\nOrganizing complaint_types.")
cat("\n\n********** COMPLAINT TYPES **********")
total_rows <- nrow(d311)
# One pass: frequency + agency labeling per complaint_type
complaint_summary_dt <- d311[
, .(
count = .N,
unique_agency_count = uniqueN(agency, na.rm = TRUE),
agency = {
u <- unique(agency)
u <- u[!is.na(u)]
if (length(u) == 1L) u else if (length(u) == 0L) NA_character_ else "MULTIPLE"
}
),
by = complaint_type
][
order(-count)
][
# compute percents from counts to avoid cumulative rounding drift
, `:=`(
percent = round(100 * count / total_rows, 2),
cumulative_percent = round(100 * cumsum(count) / total_rows, 2)
)
][]
cat("\nThere are", nrow(complaint_summary_dt), "different complaint_type(s).\n")
# ---- Console reports ----
top_n <- 20L
cat("\nTop ", top_n, " complaint_type(s) and responsible agency:\n", sep = "")
top_dt <- complaint_summary_dt[1:min(.N, top_n), .(complaint_type, count, percent, cumulative_percent, agency)]
top_df <- data.frame(
complaint_type = format(top_dt$complaint_type, justify = "left"),
count = format(top_dt$count, justify = "right"),
percent = format(top_dt$percent, justify = "right"),
cumulative_percent = format(top_dt$cumulative_percent, justify = "right"),
agency = format(top_dt$agency, justify = "left")
)
print(top_df, row.names = FALSE)
cat("\nBottom ", top_n, " complaint_type(s) and responsible agency:\n", sep = "")
bottom_dt <- tail(complaint_summary_dt[, .(complaint_type, count, agency)], top_n)
bottom_df <- data.frame(
complaint_type = format(bottom_dt$complaint_type, justify = "left"),
count = format(bottom_dt$count, justify = "right"),
agency = format(bottom_dt$agency, justify = "left")
)
print(bottom_df, row.names = FALSE)
cat("\nComplaints with multiple responsible agencies:\n")
multiple_agency_dt <- complaint_summary_dt[agency == "MULTIPLE", .(complaint_type, count, percent)]
multiple_df <- data.frame(
complaint_type = format(head(multiple_agency_dt, top_n)$complaint_type, justify = "left"),
count = format(head(multiple_agency_dt, top_n)$count, justify = "right"),
percent = format(head(multiple_agency_dt, top_n)$percent, justify = "right")
)
print(multiple_df, row.names = FALSE)
# ---- Noise complaints (prefix "NOISE") ----
noise_dt <- complaint_summary_dt[startsWith(complaint_type, "NOISE")]
cat("\nThere are", nrow(noise_dt), "categories of noise complaints:\n")
noise_display_dt <- noise_dt[, .(complaint_type, count, percent)]
noise_df <- data.frame(
complaint_type = format(noise_display_dt$complaint_type, justify = "left"),
count = format(noise_display_dt$count, justify = "right"),
percent = format(noise_display_dt$percent, justify = "right")
)
print(head(noise_df, top_n), row.names = FALSE)
noise_total <- noise_dt[, sum(count)]
noise_pct   <- round(100 * noise_total / total_rows, 1)
cat(
"\nNoise complaints of all ", nrow(noise_dt), "types number ",
format(noise_total, big.mark = ","),
", constituting ", noise_pct, "% of all SRs.\n", sep = ""
)
# chart
plot_pareto_combo(
DT              = d311,
x_col           = complaint_type,
title           = "Pareto Analysis of Complaint Types",
filename        = "SR_by_complaint_type_pareto_combo_chart.pdf",
chart_dir       = chart_dir,
show_labels     = FALSE,
top_n            = 20,
show_threshold_80 = FALSE,   # whether to draw the 80% reference line
annotation_size = 3
)
source("~/datacleaningproject/journal_of_data_science/nyc_311_data_cleaning/code/jds_datacleansings.R")
source("~/datacleaningproject/journal_of_data_science/nyc_311_data_cleaning/code/jds_datacleansings.R")
################################################################################
# Read-in raw csv data (main_data_file)
# Convert text columns to upper case
# Convert date columns to POSIXct format
# Modify and standardize column names
# Check for presence of mandatory fields
# Combine NYC Agencies to accommodate name changes
# Replace missing values with NA for standardization
# Write out two files in RDS format and one in CSV
################################################################################
main_data_file <- "raw_data_5_years_AS_OF_10-10-2025.csv"
# Boolean flag. TRUE to redirect console output to text file
# FALSE to display console outpx`t on the screen
enable_sink <- TRUE
# Okabe-Ito palette for colorblind safe
palette(c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
"#0072B2", "#D55E00", "#CC79A7", "#999999"))
################################################################################
# ------------------------------------------------------ -------
# ðŸ“¦ CREATE REQUIRED DIRECTORY STRUCTURE
# -------------------------------------------------------------
################################################################################
# Main Analysis Script
################################################################################
# STEP 1: Create directory structure (inline)
# Set base directory to current working directory
base_dir <- getwd()
cat("Base directory:", base_dir, "\n")

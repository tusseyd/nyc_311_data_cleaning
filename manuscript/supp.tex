\documentclass[linenumber]{jdsart}
\pdfminorversion=7

\usepackage{siunitx}
\sisetup{
    group-separator = {,},
    round-mode = places,
    round-precision = 2,
    output-decimal-marker = {.},
    table-number-alignment = center,
    table-figures-integer = 6,
    table-figures-decimal = 2,
    table-figures-uncertainty = 2
}

\usepackage{comment}
\usepackage{multicol}
%\usepackage{fontawesome5}
%\usepackage{booktabs, textgreek}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{dirtree}
\usepackage{tikz}
\usetikzlibrary{positioning}

% image path
\graphicspath{{./}{./supplemental_images/}}

% =========================================================
% AUTO SUPPLEMENT NUMBERING (Tables/Figures as S1, S2, ...)
% =========================================================
\makeatletter
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\fnum@table}{\tablename~\thetable}
\renewcommand{\fnum@figure}{\figurename~\thefigure}
\makeatother

%% float control
\renewcommand\floatpagefraction{0.75}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

% hyperref settings (safe to keep)
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\hyphenpenalty=950

\volume{0}
\issue{0}
\pubyear{2025}
\doi{0000}


\begin{document}

\begin{frontmatter}

\title{\Large Supplementary Materials for:}
\subtitle{\large Principles for Open Data Curation: A Case Study with the New York City 311 Service Request Data\\[2ex]}

\author[1]{\inits{D.}\fnms{David}~\snm{Tussey}}
\author[2]{\inits{J.}\fnms{Jun}~\snm{Yan}}

\runtitle{Supplementary Materials: NYC 311 Data Cleansing}
\runauthor{Tussey and Yan}

\address[1]{\institution{NYC Department of Information Technology and Telecommunications}, \cny{USA}}
\address[2]{Department of Statistics, \institution{University of Connecticut}, \cny{USA}}

\begin{keywords}
  \kwd{Data cleansing}
  \kwd{Data quality}
  \kwd{Data science}
  \kwd{Data Validation}
  \kwd{NYC Open Data}
\end{keywords}

\end{frontmatter}

%---------------------------------------------------------------
\section{Overview}

This document provides supplementary materials accompanying the article
\emph{Principles for Open Data Curation: A Case Study with the New York City 
311 Service Request Data}. It includes additional figures, tables, extended 
methodological details, data-processing steps, selected console output, 
and reproducibility information that support—but are not included 
in—the main manuscript.

The supplementary materials are intended to improve transparency,
facilitate reproducibility, and provide additional technical context
for readers interested in the data-cleaning and validation processes
applied in this study.

As noted in the \emph{Principles for Open Data Curation: A Case Study with the
New York City 311 Service Request Data} there are two separate programs which 
should be executed in order:

\vspace{\baselineskip} 
\begin{enumerate}[leftmargin=4em]
\item \texttt{data\_prep\_for\_jds\_datacleansing.R} --- Prepares the raw data for analysis
\item \texttt{jds\_datacleansings.R} --- Performs data cleansing and quality assessments
\end{enumerate}

\vspace{\baselineskip} 
Each of these two programs produces a console output file in simple text format, specifically:
\texttt{JDS\_data\_prep\_console\_output.txt} and \texttt{JDS\_datacleaning\_console\_output.tex}.
These two files, along with the associated charts provide additional insight 
into the anomalies observed during data cleansing. Additionally, upon running 
the \texttt{jds\_datacleansings.R}, there will be 81 charts in .PDF format in 
the \texttt{charts} directory and a \texttt{field\_usage\_summary\_table.csv}
file in the \texttt{analytics} directory. 


The below examples represent typical charts and tables created by the 
R programs. A full listing of the tables produced is available in the 
reference text files stored on GitHub. The charts shown below are
representative of the types of anomalies discovered and the associated
Pareto and box plot charts visualizing each. These charts are not
available online, but the underlying data is captured in the tables in
the console output text files.  


%---------------------------------------------------------------
\section{Example Supporting Charts}


Many of the charts are Pareto charts typically showing a selected metric,
e.g. \textit{invalid community\_boards} broken out by NYC Agencies. These
Pareto charts often reveal which Agency is contributing significantly to the 
anomaly, thus identifying a potential point for corrective action. Additionally, 
the Pareto chart often reveals a distribution by Agency that roughly 
corresponds to the overall 311 Service Request (SR) distribution, thus potentially 
indicating a systemic problem, rather than an Agency-specific issue.


\subsection{Status and Closure Consistency Analysis}


Here is a classic example. In this case, the anomaly is an SR that has a \texttt{status}
of \textit{closed}, but is missing a \texttt{closed\_date} entry. The Pareto chart 
clearly shows that this is a problem located almost exclusively with the NYC
Department of Homeless Services (DHS), as 99.7\% occur there.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{pareto_closed_status_missing_closed_date_by_agency.pdf}
\caption{Pareto chart of CLOSED status but missing \texttt{closed\_date} by agency.}
\label{fig:pareto-closed-status-missing-date}
\end{figure}


Conversely, the anomaly of having a \texttt{closed\_date}, but not having a
\texttt{status} of CLOSED is shown to exist primarily in the Department of
Transportation (DOT)---77\%, with smaller anomalies in the Department of
Buildings (DOB) and the Department of Sanitation (DSNY).

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{pareto_nonclosed_status_with_closed_date_by_agency.pdf}
\caption{Pareto chart of NOT-CLOSED status but has \texttt{closed\_date} by agency.}
\label{fig:pareto-nonclosed-with-date}
\end{figure}


\subsection{Temporal Pattern Analysis}


The charts can also reveal trends in an anomaly. As noted in the reference document,
there is a pronounced spike in the number of SRs that are closed \emph{exactly} at 
midnight (00:00:00). This spike potentially indicates a automated bulk-action software program
that closes outstanding SRs exactly at midnight. But as shown in the chart of the yearly 
distribution of midnight \texttt{closed\_date)}(s), this anomaly appears to be 
decreasing in the more current years. Here is the calendar year distribution of 
\emph{closed exactly at midnight} SRs.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{closed_exact_midnight_cy_distribution.pdf}
\caption{Calendar year distribution of midnight \texttt{closed\_date} entries.}
\label{fig:closed-exact-midnight-cy}
\end{figure}


And supplementing that discovery is the Pareto chart of the midnight anomaly by 
Agency. Here we can discern that the majority of the SRs closed at midnight
occur withing two Agencies, DSNY and DOB, which collectively are responsible 
for 97.6\% of such anomalies.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{pareto_closed_exact_midnight_agency.pdf}
\caption{Pareto chart of midnight \texttt{closed\_date} entries by agency.}
\label{fig:pareto-closed-midnight}
\end{figure}


\subsection{Service Request (SR) Duration Analysis}


A number of the charts are box plots, illustrating not only the 
prevalence of an anomaly, but also the extent. Such box plots capture 
numeric metrics and are arranged in descending order by the 
count of anomalies. They show both the distribution by Agency, and 
also the extent. Here, the box plot shows the duration 
(response time) calculated as $\texttt{closed\_date} - \texttt{created\_date}$. The 
anomaly being measured is SRs that have large positive durations, in this
case between two and five years, limits that seem reasonable to count 
as \emph{long}. There are over 1 million of these SRs
that take a long, long time to complete. The box plot shows that 
the Department of Parks and Recreation (DPR) originates the most 
of these large durations, followed closely by the Department
of Health and Mental Hygiene (DOHMH), the Economic Development 
Council (EDC) and DSNY. Not only do these four Agencies contribute
the most to the long-duration metric, but the mean of such duration 
and the spread are also quite high. Meanwhile, the NYPD, has
relatively few such large durations (response times) and those 
are limited in magnitude with the mean \emph{long} duration being just over the 
two year lower limit at 771 days and a tight $\sigma$ of 79. Other Agency's
$\sigma$ values are 2-4X that magnitude, indicating a large spread. All Agency values 
can be obtained by observing the associated box plot chart, supplemented by two
tables produced in the console output \texttt{.tex} file, shown below.


\begin{table}[htbp]
\centering
\caption{Pareto Summary by Agency}
\label{tab:pareto_agency}
\begin{tabular}{lrrr}
\toprule
Agency & N & Percent & Cumulative Percent \\
\midrule
DPR   & 56,372 & 0.40 & 0.40 \\
DOHMH & 42,800 & 0.31 & 0.71 \\
EDC   & 32,529 & 0.23 & 0.94 \\
DSNY  &  2,816 & 0.02 & 0.96 \\
DOB   &  2,236 & 0.02 & 0.98 \\
DOT   &  1,727 & 0.01 & 0.99 \\
HPD   &    418 & 0.00 & 0.99 \\
DEP   &    351 & 0.00 & 1.00 \\
DOE   &    189 & 0.00 & 1.00 \\
TLC   &    142 & 0.00 & 1.00 \\
DHS   &     25 & 0.00 & 1.00 \\
NYPD  &     20 & 0.00 & 1.00 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[htbp]
\centering
\caption{Summary Statistics for Large Positive Duration (days) by Agency}
\label{tab:duration_summary}
\begin{tabular}{lrrrrrr}
\toprule
Agency & N & Min & Median & Max & Mean & $\sigma$ \\
\midrule
DCWP  &     3 & 1200.09 & 1230.18 & 1233.24 & 1221.17 &  18.32 \\
DEP   &   351 &  731.49 &  958.19 & 1814.23 & 1046.57 & 282.60 \\
DHS   &    25 &  770.04 & 1377.21 & 1741.76 & 1382.27 & 310.87 \\
DOB   &  2236 &  730.61 &  860.76 & 1816.18 &  930.60 & 205.65 \\
DOE   &   189 &  731.17 &  919.86 & 1178.73 &  931.37 & 118.70 \\
DOHMH & 42800 &  730.56 & 1148.68 & 1824.99 & 1163.16 & 271.48 \\
DOT   &  1727 &  730.55 &  919.70 & 1773.78 & 1005.19 & 250.10 \\
DPR   & 56372 &  730.62 & 1020.23 & 1826.21 & 1093.54 & 287.95 \\
DSNY  &  2816 &  730.54 & 1177.62 & 1826.06 & 1133.12 & 164.42 \\
EDC   & 32529 &  731.56 & 1064.31 & 1356.85 & 1041.29 & 183.81 \\
HPD   &   418 &  731.04 &  952.37 & 1573.61 &  978.80 & 177.81 \\
NYPD  &    20 &  734.48 &  737.93 & 1044.58 &  771.66 &  79.43 \\
TLC   &   142 &  730.81 &  845.01 & 1371.10 &  909.25 & 156.38 \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{boxplot_large_positive_days_by_agency.pdf}
\caption{Distribution of large positive durations (730--1{,}826 days) by agency.}
\label{fig:boxplot-large-positive-durations}
\end{figure}


A similar situation is illustrated by the box plot showing negative durations, that is
where the \texttt{closed\_date} occurs before the \texttt{created\_date} resulting 
in a nonsensical negative duration.  In this box plot, we and see that DOT is
the largest offender, but the mean negative durations are far larger for the 
Department of Environmental Protection (DEP) as shown on a logarithmic x-axis.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{boxplot_negative_days_by_agency.pdf}
\caption{Distribution of negative durations (closed $<$ created date) by agency)}
\label{fig:boxplot-negative-durations}
\end{figure}


Violin charts of these same distributions are also produced and 
available in the \texttt{charts} directory.) Here is the violin chart that 
accompanies the negative duration box plot, revealing the density
of negative durations by magnitude.



\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{violin_boxplot_negative_days_by_agency.pdf}
\caption{Combined violin and boxplot for negative durations by agency.}
\label{fig:violin-boxplot-negative}
\end{figure}


\subsubsection{Negative, Zero, and Near-Zero Duration Visualizations}


One challenging area was determining those duration values that are 
impossible, improbable, or suspicious. Some duration categories are obvious, such as a
zero second duration, i.e. the \texttt{crerated\_date} and the \texttt{closed\_date}
are identical, to the second. There are 387{,}329 such impossible duration SRs. Additionally, there
are 3460 SRs with a duration of exactly one second, highly improbable if not impossible. Finally, 
there are those SRs that have durations that are quite small, i.e. just a few seconds. As noted
in the referenced document, a quantitative analysis revealed a boundary of 28 seconds as
an outlier (LogNormal\_$3\sigma$). The results of this analysis are shown sorted
by the detection threshold method. Using this value reveals another 14$,$741 suspicious durations.
Here are the charts that capture the analysis of this suspicious boundary. Two charts
visualize the overall distribution of low duration SRs from 2---90 seconds; one 
cumulative and one show durations by count. The cumulative distribution 
shows the suspicious threshold of 28 seconds representing approximately 12\% of
SRs with durations inside that narrow time span. 


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{skewed_duration_analysis_methods.pdf}
\caption{Outlier Detection Thresholds by Method. LogNormal\_3SD returned a useable result.}
\label{fig:zero-duration-pareto-combo}
\end{figure}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{duration_histogram_cumulative.pdf}
\caption{Cumulative Distribution of SR durations (2 to 90 seconds.}
\label{fig:zero-duration-pareto-combo}
\end{figure}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{duration_histogram.pdf}
\caption{Distribution of SR durations (2 to 90 seconds) by count.}
\label{fig:zero-duration-pareto-combo}
\end{figure}


\paragraph{Interpretation}
The duration data exhibits extreme right-skewness, with a mean/median ratio of 
53.96 in the raw data, indicating that a small number of extremely long-duration 
cases dominate the distribution. After truncating at 2 days (172,800 seconds) to 
focus on short-duration analysis, approximately 35\% of records were excluded, 
and the mean/median ratio improved to 5.19, though still indicating substantial 
skew. Traditional outlier detection methods based on standard deviations, median 
absolute deviation (MAD), and interquartile range (IQR) failed to identify any 
thresholds, returning zero outliers due to the extreme concentration of values at 
the low end of the distribution. Only log-normal and percentile-based methods 
successfully identified potential short-duration thresholds. The LogNormal 3SD 
method suggests a threshold of 28 seconds, below which lie 10,231 records 
(0.07\%)—cases that are statistically unusual even accounting for the log-normal 
distribution characteristic of administrative process times. Among the 93,408 
records with durations between 2--90 seconds, the median of 57 seconds 
represents cases resolved in under one minute, likely reflecting automated closures, 
duplicate entries, or requests immediately identified as misdirected rather 
than genuine service delivery times.


%---------------------------------------------------------------
\subsection{Field usage by agency}


The complete field-by-agency usage matrix is provided as a machine-readable
comma-separated values (CSV) file due to its size and dimensionality.
The table records the number of non-missing observations for each data
field, stratified by reporting agency, across the full NYC~311 dataset
used in this study.

The CSV file is available at:
\begin{quote}
\url{https://figshare.com/s/9f878b50687c9e4c540a}
\end{quote}

Each row corresponds to a single data field, and each column corresponds
to a reporting agency. Cell values indicate the count of records in which
the given field is populated for the specified agency. The final column
contains the total count across all agencies.

\paragraph{Interpretation.}
Field usage varies substantially by agency, reflecting differences in
operational responsibilities, reporting practices, and data-collection
requirements. Core administrative fields (e.g., \texttt{unique\_key},
\texttt{created\_date}, \texttt{agency}, and \texttt{complaint\_type})
are fully populated across all agencies, whereas location-specific,
infrastructure-related, and program-specific fields are populated only
by agencies for which they are operationally relevant.


%---------------------------------------------------------------
\subsection{Multi-year statistics (Actuals + Projected)}


\begin{table}[!htbp]
\centering
\begin{tabular}{lr}
\toprule
Statistic & Value \\
\midrule
Years covered & 2020--2025 \\
Total records & 19{,}525{,}499 \\
Yearly mean & 3{,}254{,}250 \\
Yearly median & 3{,}223{,}236 \\
Std.\ dev.\ & 206{,}378 \\
Max year & 2025 (3{,}508{,}799) \\
Min year & 2020 (2{,}942{,}064) \\
Growth (\%) & 19.3 \\
Busiest month & 2020-08 (348{,}463) \\
Least busy month & 2020-04 (159{,}115) \\
Busiest day & 2020-08-04 (24{,}415) \\
Least busy day & 2020-03-29 (3{,}785) \\
\bottomrule
\end{tabular}
\caption{Multi-year request-volume statistics for 2020--2025 (including projected 2025 total).}
\label{tab:multiyear-stats-2020-2025}
\end{table}

\paragraph{Interpretation.}
Volume increases over the period, with a clear low point in
early 2020 and a later peak in 2025 (projected). It is interesting to note 
that the least busy day and the busiest day both occur in 2020. It is 
suspected this is related to the COVID pandemic, which saw large layoffs 
and closures in NYC beginning in the March/April time frame, while the 
busiest day likely corresponds to the emergence of COVID testing sites and
a gradual return to work, schools opening, etc. These aggregates help
contextualize agency and field-coverage patterns by overall workload.


%---------------------------------------------------------------
\section{Reproducibility and Code Access}


\subsection*{System Requirements}


\begin{itemize}
\item R version 4.5.2 or higher (\url{https://cran.r-project.org/})
\item RStudio version 2026.01.0 build 392 or higher (recommended): \\
\url{https://posit.co/download/rstudio-desktop/}
\item Minimum RAM: 16 GB (tested and verified; 32 GB provides faster execution)
\item Disk space: ~15 GB (9 GB uncompressed data + intermediate files + outputs)
\item Operating System: Tested on Windows 10/11; macOS/Linux users will need to 
modify file paths in \texttt{base\_dir}
\end{itemize}


\subsection*{Data and Code Repository}


\begin{itemize}
\item Source code: \url{https://github.com/tusseyd/nyc_311_data_cleaning}
\item NYC 311 dataset (1.57 GB zipped, 9 GB uncompressed): \\
\url{https://figshare.com/s/e8d479c391edb7224bfa}
\item USPS Zip Codes (4.5 MB): \url{https://figshare.com/s/8aea027d06f4903f2227}
\item Reference console output for validation is available in GitHub repository.
\end{itemize}

\subsection*{Required R Packages}
Both scripts automatically check for and install missing packages. The analysis 
requires: \texttt{fasttime}, 
\texttt{clock}, \texttt{zoo}, \texttt{lubridate}, \texttt{sf}, \texttt{stringr}, \texttt{stringdist}, 
\texttt{arrow}, \texttt{ggplot2}, \texttt{dplyr}, \texttt{tidyverse}, \texttt{ggpmisc}, 
\texttt{gridExtra}, \texttt{grid}, \texttt{qcc}, \texttt{qicharts2}, \texttt{gt}, \texttt{DT}, 
\texttt{bslib}, \texttt{shiny}, \texttt{httr}, \texttt{rlang}, \texttt{styler}, 
\texttt{renv}, and \texttt{data.table}.


\subsection*{Analysis Reproduction Steps}


Source files are available at:
\begin{itemize}
  \item \textsc{R} source code (GitHub): \url{https://github.com/tusseyd/nyc_311_data_cleaning}
  \item USPS Zip Code dataset (Figshare): \url{https://figshare.com/s/8aea027d06f4903f2227}
  \item NYC 311 dataset (Figshare): \url{https://figshare.com/s/e8d479c391edb7224bfa}
\end{itemize}
\vspace{0.5cm}
Step 1: Download Data Files
\begin{enumerate}
\item Download the NYC 311 and USPS Zip Code datasets from Figshare. (links above)
\item Do not rename these files as the scripts expect original filenames:
\begin{itemize}
\item \texttt{5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.zip}
\item \texttt{zip\_code\_database.csv}
\end{itemize}
\item Extract the CSV from the zip file ($\sim$9 GB uncompressed)
\end{enumerate}
\vspace{0.5cm}
Step 2: Set Up Project Structure
\begin{enumerate}
\item Clone or download the entire GitHub repository: \\
\url{https://github.com/tusseyd/nyc_311_data_cleaning}
\item Important: Ensure you download all files including:
\begin{itemize}
\item \texttt{code/data\_prep\_for\_jds\_datacleansing.R}
\item \texttt{code/jds\_datacleansing.R}
\item All files in \texttt{code/functions/} directory (required dependencies)
\end{itemize}
\item The data preparation script will automatically create the following directory structure:
\vspace{0.5cm}
\dirtree{%
.1 nyc\_311\_data\_cleaning/ (R Working Directory).
.2 analytics/.
.3 field\_usage\_summary\_table.csv.
.2 charts/.
.3 [82 PDF files].
.2 code/.
.3 data\_prep\_for\_jds\_datacleansing.R.
.3 jds\_datacleansing.R.
.3 functions/.
.4 [function files].
.2 console\_output/.
.3 JDS\_data\_prep\_console\_output.txt.
.3 JDS\_datacleansing\_console\_output.txt.
.2 data/.
.3 5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.rds.
.3 USPS\_zipcodes.rds.
.3 raw\_data/.
.4 5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.csv.
.4 zip\_code\_database.csv.
.2 reference\_outputs/.
.3 JDS\_data\_prep\_console\_output.txt.
.3 JDS\_datacleansing\_console\_output.txt.
}
\vspace{0.5cm}
Step 3:  Open RStudio and open \texttt{code/data\_prep\_for\_jds\_datacleansing.R}
Modify the \texttt{base\_dir} variable (line 85) to match your local path:
\begin{verbatim}
base_dir <- file.path("your", "path", "here", 
                      "nyc_311_data_cleaning")
\end{verbatim}
\textit{Note: Non-Windows users should use forward slashes or platform-appropriate path separators}

\vspace{0.5cm}
Step 4: Place the downloaded data files in the following directories:
\begin{itemize}
\item \texttt{5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.csv} $\rightarrow$ \texttt{data/raw\_data/}
\item \texttt{zip\_code\_database.csv} $\rightarrow$ \texttt{data/raw\_data/}
\end{itemize}
\end{enumerate}
\vspace{0.5cm}
Step 5: Run the Data Preparation R program
\begin{enumerate}
\item Execute \texttt{data\_prep\_for\_jds\_datacleansing.R} in RStudio
\item The script will automatically create directory structure and install missing packages
\item \textit{Expected runtime: Approximately 90 minutes on a system with 16 GB RAM}
\item \textit{Note: Initial data loading is memory-intensive; the script optimizes memory usage after initial processing}
\item Upon completion, verify these files exist:
\begin{itemize}
\item \texttt{data/5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.rds}
\item \texttt{data/USPS\_zipcodes.rds}
\item \texttt{console\_output/JDS\_data\_prep\_console\_output.txt}
\end{itemize}
\end{enumerate}
\vspace{0.5cm}
Step 6: Run Data Cleansing Analysis R program
\begin{enumerate}
\item Execute \texttt{code/jds\_datacleansing.R} in RStudio
\item \textit{Expected runtime: Approximately 60 minutes on a system with 16 GB RAM}
\item Monitor progress in RStudio's plot pane as charts are generated
\item The script will produce:
\begin{itemize}
\item 82 PDF charts in \texttt{charts/}
\item \texttt{field\_usage\_summary\_table.csv} in \texttt{data/analytical\_files/}
\item \texttt{console\_output/JDS\_datacleaning\_console\_output.txt}
\end{itemize}
\end{enumerate}

\subsection*{Validation}
Compare your console output files against the reference outputs provided in the 
GitHub repository to verify successful reproduction. The console outputs contain 
detailed execution logs, row counts, and session information.

\subsection*{Notes}
\begin{itemize}
\item Total processing time: Approximately 2.5 hours on a system with 16 GB RAM; 
faster on systems with 32 GB
\item The scripts use \texttt{data.table} for memory-efficient processing of 10+ million records
\item All file paths are configured relative to \texttt{base\_dir} for portability
\item Memory management: Initial data loading requires substantial RAM; the script 
reduces memory footprint after preprocessing
\end{itemize}


\subsection*{Validation}
Compare your console output files against the reference outputs provided 
in the GitHub repository to verify successful reproduction. The console outputs contain
tables of information which should match your run. 



\end{document}

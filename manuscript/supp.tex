\documentclass[linenumber]{jdsart}
\pdfminorversion=7

\usepackage{siunitx}
\sisetup{
    group-separator = {,},
    round-mode = places,
    round-precision = 2,
    output-decimal-marker = {.},
    table-number-alignment = center,
    table-figures-integer = 6,
    table-figures-decimal = 2,
    table-figures-uncertainty = 2
}

\usepackage{comment}
\usepackage{multicol}
%\usepackage{fontawesome5}
%\usepackage{booktabs, textgreek}
\usepackage{booktabs}  % for \toprule, \midrule, \bottomrule
\usepackage{threeparttable}  % for table notes
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{dirtree}
\usepackage{tikz}
\usetikzlibrary{positioning}

% image path
\graphicspath{{./}{./supplemental_images/}}

% =========================================================
% AUTO SUPPLEMENT NUMBERING (Tables/Figures as S1, S2, ...)
% =========================================================
\makeatletter
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\fnum@table}{\tablename~\thetable}
\renewcommand{\fnum@figure}{\figurename~\thefigure}
\makeatother

%% float control
\renewcommand\floatpagefraction{0.75}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

% hyperref settings (safe to keep)
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\hyphenpenalty=950

\volume{0}
\issue{0}
\pubyear{2025}
\doi{0000}


\begin{document}

\begin{frontmatter}

\title{\Large Supplementary Materials for:}
\subtitle{\large Principles for Open Data Curation: A Case Study with the New York City 311 Service Request Data\\[2ex]}

\author[1]{\inits{D.}\fnms{David}~\snm{Tussey}}
\author[2]{\inits{J.}\fnms{Jun}~\snm{Yan}}

\runtitle{Supplementary Materials: NYC 311 Data Cleansing}
\runauthor{Tussey and Yan}

\address[1]{\institution{NYC Department of Information Technology and Telecommunications}, \cny{USA}}
\address[2]{Department of Statistics, \institution{University of Connecticut}, \cny{USA}}

\begin{keywords}
\kwd{Data cleansing}
\kwd{Data quality}
\kwd{Data science}
\kwd{Data Validation}
\kwd{NYC Open Data}
\end{keywords}

\end{frontmatter}

% Table of contents and lists
\tableofcontents
\listoffigures
\listoftables
\newpage


%---------------------------------------------------------------
\section{Overview}

This document provides supplementary materials accompanying the article
\emph{Principles for Open Data Curation: A Case Study with the New York City 
311 Service Request Data}. It includes additional figures, tables, extended 
methodological details,  selected console output, and reproducibility 
information that support—but are not included in—the main manuscript.

The supplementary materials are intended to improve transparency,
facilitate reproducibility, and provide additional technical context
for readers interested in the data-cleaning and validation processes
applied in this study.


\begin{itemize}
\item Data preparation: \\ \small\url{https://github.com/tusseyd/nyc_311_data_cleaning/blob/main/console_output/reference_JDS_data_prep_console_output.txt}
\vspace{0.1cm}
\item Data cleaning: \\ \small\url{https://github.com/tusseyd/nyc_311_data_cleaning/blob/main/console_output/reference_JDS_datacleaning_console_output.txt}
\end{itemize}

\vspace{0.1cm}
The below examples represent typical charts and tables created by the 
R programs. A full listing of the tables produced is available in the 
\emph{reference} named text files stored on GitHub. The charts shown below are
representative of the types of anomalies discovered and the associated
Pareto and box plot visualizations. 


%---------------------------------------------------------------
\section{Example Supporting Charts}


Many of the charts are Pareto charts displaying selected quality metrics or 
anomalies, such as \textit{invalid community\_boards}, stratified by NYC 
Agency. These Pareto charts often reveal which Agency is contributing most 
significantly to the anomaly, thus identifying a potential point for corrective 
action. Sometimes, the Pareto chart reveals a distribution by Agency that roughly 
corresponds to the overall 311 Service Request (SR) distribution, thus potentially 
indicating a systemic problem, rather than an Agency-specific issue. Some 
illustrative Pareto charts are described below.


\subsection{Status and Closure Consistency Analysis}


This is a classic example of identifying the source Agency associated with an anomaly. 
In this case, the anomaly is an SR that has a \texttt{status}
of \textit{closed}, but is missing the \texttt{closed\_date} entry. The Pareto chart 
clearly reveals that this is a problem located almost exclusively within the NYC
Department of Homeless Services (DHS), as 99.7\% such anomalies occur there.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{pareto_closed_status_missing_closed_date_by_agency.pdf}
\caption{Pareto chart of SRs with a CLOSED status, but missing \texttt{closed\_date}, by Agency.}
\label{fig:pareto-closed-status-missing-date}
\end{figure}


Conversely, the anomaly of having a \texttt{closed\_date}, but \emph{not} having a
\texttt{status} of CLOSED is shown to primarily exist within the Department of
Transportation (DOT)---a full 77\%, with smaller anomalies occurring within the Department of
Buildings (DOB) and the Department of Sanitation (DSNY).


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{pareto_nonclosed_status_with_closed_date_by_agency.pdf}
\caption{Pareto chart of SRs with NOT-CLOSED status, but has a \texttt{closed\_date}, by Agency.}
\label{fig:pareto-nonclosed-with-date}
\end{figure}


\subsection{Temporal Pattern Analysis}


The charts  also reveal trends in an anomaly. As noted in the reference manuscript,
there is a pronounced spike in the number of SRs closed \emph{exactly} at 
midnight (00:00:00). This spike potentially indicates a automated bulk-action 
software program that seems to close selected SRs exactly at midnight. But 
as shown in the chart of the yearly distribution of midnight \texttt{closed\_date)}(s), 
this anomaly appears to be decreasing in the more current years. 


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{closed_exact_midnight_cy_distribution.pdf}
\caption{Calendar year distribution of SRs with a midnight \texttt{closed\_date}.}
\label{fig:closed-exact-midnight-cy}
\end{figure}


Supplementing that discovery is the Pareto chart of the midnight anomaly by 
Agency. The chart clearly shows that the majority of the SRs closed-at-midnight
occur within two Agencies, DSNY and DOB, which collectively are responsible 
for 97.6\% of such anomalies.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{pareto_closed_exact_midnight_agency.pdf}
\caption{Pareto chart of SRs with a midnight \texttt{closed\_date}, by Agency.}
\label{fig:pareto-closed-midnight}
\end{figure}


\subsection{Service Request (SR) Positive Duration Analysis}


A number of charts are box plots, illustrating not only the 
prevalence of an anomaly, but also the extent. Box plots capture 
numeric metrics and are always presented in descending order by 
count of the anomaly. They show both the distribution by Agency, and 
also the extent. 


The box plot below shows duration 
(response time) calculated as $\texttt{closed\_date} - \texttt{created\_date}$. The anomaly 
measured is SRs that have large positive durations, in this
case between two and five years (730--1825 days) --- duration limits that 
seem reasonable to count as \emph{large}. There are over 1 million such 
SRs that take a long time to resolve and be marked as Closed. 
For example, the box plot shows that 
the Department of Parks and Recreation (DPR) is responsible for most 
of these large durations, followed closely by the Department
of Health and Mental Hygiene (DOHMH), the Economic Development 
Council (EDC) and DSNY. Not only do these four Agencies contribute
significantly to the long-duration SR metric, but the statistical mean of such 
durations, and the spread are also quite high. Meanwhile, as the box plot reveals, 
the NYPD, has relatively few large durations and those are limited in magnitude 
with the mean duration being just over the 
two year lower limit at 771 days and a tight $\sigma$ of 79. Other Agency's
$\sigma$ values are 2-4X that magnitude, indicating a broader spread. All Agency values 
can be obtained by observing the associated box plot chart, supplemented by two
tables produced in the console output \texttt{.tex} file, shown below.


Further insight emerges from analyzing all SRs with positive durations, regardless of 
magnitude---15.3 million observations (94\% of the dataset). Figure~\ref{fig:bimodal_hist} 
presents a histogram of these durations on a logarithmic scale, revealing a 
bimodal distribution. Hartigan's dip test confirms this bimodality is statistically 
significant ($D = 0.027$, $p < 2.2 \times 10^{-16}$), decisively rejecting the 
null hypothesis of unimodality.

To objectively identify the separation threshold between these operational modes, 
we applied kernel density estimation to log-transformed durations and 
located the local minimum between the two prominent peaks. This valley-based 
approach yielded a threshold of 0.432 days (10.4 hours), providing a reproducible 
criterion for classifying service requests as rapid-resolution ($< 0.432$ days) 
versus standard work orders ($\geq 0.432$ days).


Table~\ref{tab:mode_summary} presents summary statistics for each operational mode. 
The rapid-resolution mode comprises 49.9\% of requests (n = 7.66M) with a 
median duration of 0.87 hours and mean of 1.72 hours. The standard work 
order mode accounts for 50.1\% of requests (n = 7.69M) with a median of 4.21 
days and mean of 47.3 days. This near-perfect 50/50 split masks dramatic 
operational heterogeneity across city agencies.


Table~\ref{tab:agency_mode} reveals that NYPD dominates the rapid-resolution mode, 
accounting for 87.2\% of all fast closures (6.69M of 7.66M requests). Critically, 96.8\% 
of NYPD's service requests close within 10.4 hours, reflecting observe-and-clear 
patrol responses to quality-of-life complaints where officers verify conditions and 
close tickets when no actionable violation is found. In stark contrast, infrastructure 
and regulatory agencies operate primarily in standard mode: HPD (96.1\% standard), 
DSNY (92.9\% standard), DOB (91.1\% standard), and DPR (90.1\% standard) handle 
work orders requiring physical service delivery, construction permits, code 
enforcement, or ongoing case investigation.

This bimodal structure has critical implications for data quality assessment and 
performance measurement. The aggregate mean duration of 23.7 days is a 
misleading summary statistic---it falls between the two operational modes and 
represents neither reality. Any citywide analysis of 311 response times that does 
not stratify by agency type conflates fundamentally different service delivery models. 
Comparing NYPD's median 52-minute response to HPD's median 4.2-day response is 
methodologically inappropriate; the former measures patrol dispatch efficiency while 
the latter measures construction project timelines. Performance metrics, anomaly 
detection thresholds, and quality benchmarks must account for this operational 
heterogeneity to avoid spurious findings and invalid cross-agency comparisons.


% Table 1: Summary statistics by operational mode
\begin{table}[htbp]
\centering
\caption{Summary Statistics by Operational Mode}
\label{tab:mode_summary}
\begin{tabular}{lrrrrrr}
\toprule
Mode & N & \% & Median & Mean & 95th \%ile \\
 & & & (hours) & (hours) & (days) \\
\midrule
Rapid-resolution & 7,663,281 & 49.9 & 0.87 & 1.72 & 0.28 \\
Standard work order & 7,689,986 & 50.1 & 101.1 & 1,135.0 & 211.0 \\
\midrule
Total & 15,353,267 & 100.0 & 51.6 & 569.0 & --- \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Note: Rapid-resolution defined as duration $< 0.432$ days (10.4 hours); 
standard work orders $\geq 0.432$ days. Median and mean for rapid-resolution 
shown in hours due to short durations; standard mode shown in hours for 
mean (47.3 days) and days for median (4.2 days) and 95th percentile.
\end{tablenotes}
\end{table}

% Table 2: Agency distribution across operational modes
\begin{table}[htbp]
\centering
\caption{Distribution of Service Requests Across Operational Modes by Agency}
\label{tab:agency_mode}
\begin{tabular}{lrrrrr}
\toprule
& \multicolumn{2}{c}{Rapid-resolution} & \multicolumn{2}{c}{Standard} & \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Agency & N & \% & N & \% & Total N \\
\midrule
NYPD & 6,685,153 & 96.8 & 220,228 & 3.2 & 6,905,381 \\
DSNY & 134,259 & 7.1 & 1,764,460 & 92.9 & 1,898,719 \\
HPD & 119,439 & 3.9 & 2,934,707 & 96.1 & 3,054,146 \\
DOT & 179,113 & 19.7 & 728,215 & 80.3 & 907,328 \\
DEP & 269,193 & 34.8 & 504,885 & 65.2 & 774,078 \\
DPR & 56,294 & 9.9 & 511,518 & 90.1 & 567,812 \\
DOB & 36,295 & 8.9 & 372,975 & 91.1 & 409,270 \\
DOHMH & 28,816 & 12.5 & 201,532 & 87.5 & 230,348 \\
DHS & 104,170 & 64.1 & 58,468 & 35.9 & 162,638 \\
EDC & 3,968 & 3.0 & 126,227 & 97.0 & 130,195 \\
DCWP & 2,373 & 2.0 & 113,744 & 98.0 & 116,117 \\
TLC & 10,119 & 8.6 & 106,997 & 91.4 & 117,116 \\
\midrule
All agencies & 7,663,281 & 49.9 & 7,689,986 & 50.1 & 15,353,267 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Note: Agencies ordered by total volume. NYPD accounts for 87.2\% of all rapid-resolution requests (6.69M of 7.66M). Percentages show within-agency distribution across modes.
\end{tablenotes}
\end{table}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{positve_duration_distribution_log.pdf}
\caption{Distribution of SRs with Positive Durations: all Agencies (log scale).}
\label{fig:histogram_all_agencies}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{nypd_only_positive_durations.pdf}
\caption{Distribution of SRs with Positive Durations: NYPD-only (log scale).}
\label{fig:histogram_NYPD}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{others_only_positive_durations.pdf}
\caption{Distribution of SRs with Positive Durations: All non-NYPD Agencies (log scale).}
\label{fig:histogram_other}
\end{figure}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{nypd_vs_others_combined.pdf}
\caption{Bimodal Distribution of  Positive Durations: NYPD and All Other Agencies (log scale).}
\label{fig:histogram_NYPD_others}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Pareto Summary by Agency ($n > 5$)}
\label{tab:pareto_agency}
\begin{tabular}{lrrr}
\toprule
Agency & N & Percent & Cumulative Percent \\
\midrule
DPR   & 56,372 & 0.40 & 0.40 \\
DOHMH & 42,800 & 0.31 & 0.71 \\
EDC   & 32,529 & 0.23 & 0.94 \\
DSNY  &  2,816 & 0.02 & 0.96 \\
DOB   &  2,236 & 0.02 & 0.98 \\
DOT   &  1,727 & 0.01 & 0.99 \\
HPD   &    418 & 0.00 & 0.99 \\
DEP   &    351 & 0.00 & 1.00 \\
DOE   &    189 & 0.00 & 1.00 \\
TLC   &    142 & 0.00 & 1.00 \\
DHS   &     25 & 0.00 & 1.00 \\
NYPD  &     20 & 0.00 & 1.00 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[htbp]
\centering
\caption{Summary Statistics for Large Positive Duration (days) by Agency}
\label{tab:duration_summary}
\begin{tabular}{lrrrrrr}
\toprule
Agency & N & Min & Median & Max & Mean & $\sigma$ \\
\midrule
DCWP  &     3 & 1200.09 & 1230.18 & 1233.24 & 1221.17 &  18.32 \\
DEP   &   351 &  731.49 &  958.19 & 1814.23 & 1046.57 & 282.60 \\
DHS   &    25 &  770.04 & 1377.21 & 1741.76 & 1382.27 & 310.87 \\
DOB   &  2236 &  730.61 &  860.76 & 1816.18 &  930.60 & 205.65 \\
DOE   &   189 &  731.17 &  919.86 & 1178.73 &  931.37 & 118.70 \\
DOHMH & 42800 &  730.56 & 1148.68 & 1824.99 & 1163.16 & 271.48 \\
DOT   &  1727 &  730.55 &  919.70 & 1773.78 & 1005.19 & 250.10 \\
DPR   & 56372 &  730.62 & 1020.23 & 1826.21 & 1093.54 & 287.95 \\
DSNY  &  2816 &  730.54 & 1177.62 & 1826.06 & 1133.12 & 164.42 \\
EDC   & 32529 &  731.56 & 1064.31 & 1356.85 & 1041.29 & 183.81 \\
HPD   &   418 &  731.04 &  952.37 & 1573.61 &  978.80 & 177.81 \\
NYPD  &    20 &  734.48 &  737.93 & 1044.58 &  771.66 &  79.43 \\
TLC   &   142 &  730.81 &  845.01 & 1371.10 &  909.25 & 156.38 \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{boxplot_large_positive_days_by_agency.pdf}
\caption{Distribution of SRs with Large Positive durations (730--1{,}826 days), by Agency.}
\label{fig:boxplot-large-positive-durations}
\end{figure}


A similar situation is illustrated by the box plot showing negative durations, that is
where the \texttt{closed\_date} occurs before the \texttt{created\_date} resulting 
in a nonsensical negative duration. In this box plot, the DOT is shown to be
the largest offender. However, the mean negative duration is far greater for the 
Department of Environmental Protection (DEP) as shown on a logarithmic x-axis. This
box plot visualization captures both the count and the extent of the 
negative duration anomaly.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{boxplot_negative_days_by_agency.pdf}
\caption{Distribution of negative durations (closed $<$ created date) by agency)}
\label{fig:boxplot-negative-durations}
\end{figure}


Violin charts of these same distributions are also produced and 
available in the \texttt{charts} directory. Here is the violin chart that 
accompanies the negative duration box plot, revealing the density
of negative durations by magnitude. Every box plot also
has an accompanying violin chart.


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{violin_boxplot_negative_days_by_agency.pdf}
\caption{Combined violin and boxplot for SRs with negative durations, by Agency.}
\label{fig:violin-boxplot-negative}
\end{figure}


\subsection{Negative, Zero, and Near-Zero Duration Visualizations}


One challenging area was determining those duration values that are 
impossible, improbable, or suspicious. Some duration categories are obvious, such as a
zero second duration, i.e. the \texttt{crerated\_date} and the \texttt{closed\_date}
are identical, to the second. There are 387{,}329 such impossible duration SRs. 
Additionally, there are 3460 SRs with a duration of exactly one second, highly improbable 
if not impossible. Finally, there are those SRs that have durations that are quite 
small, e. g. just a few seconds, and are highly suspicious. As noted
in the referenced manuscript, a quantitative analysis revealed a boundary of 28 seconds as
an outlier (LogNormal\_$3\sigma$). The results of this analysis are shown sorted
by the detection threshold method in the bar chart below. Using this value 
reveals 14$,$741 suspicious durations. Also below, are charts that capture 
the analysis of this suspicious boundary. Two charts visualize the overall 
distribution of low duration SRs from 2---90 seconds; one 
cumulative and one showing durations by count. The cumulative distribution 
shows the suspicious threshold of 28 seconds representing approximately 12\% of
SRs with durations inside that narrow 2---90 time span. 


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{skewed_duration_analysis_methods.pdf}
\caption{Outlier Detection Thresholds by Method. LogNormal\_3SD returned a useable result.}
\label{fig:zero-duration-pareto-combo}
\end{figure}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{duration_histogram_cumulative.pdf}
\caption{Cumulative Distribution of SR durations (2--90 seconds).}
\label{fig:zero-duration-pareto-combo}
\end{figure}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{duration_histogram.pdf}
\caption{Distribution of SR durations (2--90 seconds) by count.}
\label{fig:zero-duration-pareto-combo}
\end{figure}


\paragraph{Interpretation}
The duration data exhibits extreme right-skewness, with a mean/median ratio of 
53.96 in the raw data, indicating that a small number of extremely long-duration 
cases dominate the distribution. This is certainly affected by several SRs that have a
\texttt{created\_date} with a date of 1900-01-01, a possible data corruption error.
 After truncating at 2 days (172,800 seconds) to focus on short-duration analysis, 
 approximately 35\% of records were excluded, and the mean/median ratio 
 improved to 5.19, though still indicating substantial 
skew. Traditional outlier detection methods based on standard deviations, median 
absolute deviation (MAD), and interquartile range (IQR) failed to identify any 
thresholds, returning zero outliers due to the extreme concentration of values at 
the low end of the distribution. Only log-normal and percentile-based methods 
successfully identified potential short-duration thresholds. 


The LogNormal 3SD method suggests a threshold of 28 seconds, below which lie 10,231 records 
(0.07\%)—cases that are statistically unusual even accounting for the log-normal 
distribution characteristic of administrative process times. Among the 93,408 
records with durations between 2--90 seconds, the median of 57 seconds 
represents cases resolved in under one minute, likely reflecting automated closures, 
duplicate entries, or requests immediately identified as misdirected rather 
than genuine service delivery times.


%---------------------------------------------------------------
\subsection{Field usage by agency}


The complete field-usage-by-agency matrix is provided as a CSV file due 
to its size and dimensionality. The table records the number of observations 
across the full NYC~311 dataset used in this study.for each data
field, stratified by reporting Agency. Each row corresponds to a 
single data field, and each column corresponds to an NYC Agency. 
Cell values indicate the count of records in which the given field is 
populated for the specified agency. The final column
contains the total count across all agencies.


\paragraph{Interpretation.}
Field usage varies substantially by Agency, reflecting differences in
operational responsibilities, reporting practices, and data-collection
requirements. Core administrative fields (i. e., \texttt{unique\_key},
\texttt{created\_date}, \texttt{status}. \texttt{agency}, and 
\texttt{complaint\_type}) are fully populated across all 
agencies, whereas location-specific, infrastructure-related, 
and program-specific fields are populated only by agencies for 
which they are operationally relevant.

The CSV file is available at:
\begin{quote}
\url{https://figshare.com/s/9f878b50687c9e4c540a}
\end{quote}


%---------------------------------------------------------------
\subsection{Multi-year statistics (Actuals + Projected)}


\paragraph{Interpretation.}
Volume increases over the period by 19.7\%, with a clear low point in
early 2020 and a later peak in 2025 (projected). It is interesting to note 
that the least busy day and the busiest day both occur in 2020. It is 
theorized that this phenomena is reflective of COVID pandemic behavior, 
which saw large layoffs and closures in NYC beginning in the 
March/April time frame, while the busiest day likely corresponds to 
the emergence of COVID testing sites and a return to work, schools 
opening, etc. These aggregates help contextualize agency and 
field-coverage patterns by overall workload.


\begin{table}[!htbp]
\centering
\begin{tabular}{lr}
\toprule
Statistic & Value \\
\midrule
Years covered & 2020--2025 \\
Total records & 19{,}525{,}499 \\
Yearly mean & 3{,}254{,}250 \\
Yearly median & 3{,}223{,}236 \\
Std.\ dev.\ & 206{,}378 \\
Max year & 2025 (3{,}508{,}799) \\
Min year & 2020 (2{,}942{,}064) \\
Growth (\%) & 19.3 \\
Busiest month & 2020-08 (348{,}463) \\
Least busy month & 2020-04 (159{,}115) \\
Busiest day & 2020-08-04 (24{,}415) \\
Least busy day & 2020-03-29 (3{,}785) \\
\bottomrule
\end{tabular}
\caption{Multi-year request-volume statistics for 2020--2025 (includes projected 2025 data).}
\label{tab:multiyear-stats-2020-2025}
\end{table}


%---------------------------------------------------------------
\section{Reproducibility and Code Access}


\subsection*{System Requirements}


\begin{itemize}
\item R version 4.5.2 or higher (\url{https://cran.r-project.org/})
\item RStudio version 2026.01.0 build 392 or higher (recommended): \\
\url{https://posit.co/download/rstudio-desktop/}
\item Minimum RAM: 16 GB (tested and verified; 32 GB provides faster execution)
\item Disk space: $\sim$15 GB ($\sim$9 GB uncompressed data) + intermediate files + outputs)
\item Operating System: Tested on Windows 10/11; macOS/Linux users will need to 
modify file paths in \texttt{base\_dir}
\end{itemize}


\subsection*{Data and Code Repository}


\begin{itemize}
\item Source code: \url{https://github.com/tusseyd/nyc_311_data_cleaning}
\item NYC 311 dataset (($\sim$1.6 GB zipped, ($\sim$9 GB uncompressed): \\
\url{https://figshare.com/s/e8d479c391edb7224bfa}
\item USPS Zip Codes (4.5 MB): \url{https://figshare.com/s/8aea027d06f4903f2227}
\item Reference console output for validation is available in GitHub repository.
\end{itemize}


\subsection*{Required R Packages}


Both scripts automatically check for and install missing packages. The two R
programs require the following R packages:


\begin{multicols}{4}
\begin{itemize}
    \item data.table
    \item arrow
    \item fasttime
    \item here
    \item zoo
    \item ggpmisc
    \item ggpattern
    \item ggrastr
    \item qcc
    \item qicharts2
    \item grid
    \item gridExtra
    \item sf
    \item stringdist
    \item tidyverse
    \item bslib
    \item shiny
    \item DT
    \item gt
    \item styler
    \item rlang
    \item renv
    \item remotes
    \item moments
    \item diptest
\end{itemize}
\end{multicols}


\subsection*{Analysis Reproduction Steps}


\subsubsection*{Reproduction Overview}


As noted in the \emph{Principles for Open Data Curation: A Case Study with the
New York City 311 Service Request Data} document there are two separate R 
programs which should be executed in order:


\vspace{0.1cm}
\begin{enumerate}[leftmargin=4em]
\item \texttt{data\_prep\_for\_jds\_datacleansing.R} --- Prepares the raw data for analysis
\vspace{0.1cm}
\item \texttt{jds\_datacleansings.R} --- Performs data cleansing and quality assessments
\end{enumerate}
\vspace{0.1cm}
Each of these programs produces a console output file in simple text format, specifically:
\vspace{0.1cm}
\begin{itemize}
\item \texttt{JDS\_data\_prep\_console\_output.txt}
\vspace{0.1cm}
\item \texttt{JDS\_datacleaning\_console\_output.tex}
\end{itemize}
\vspace{0.1cm}
These two files, along with the associated charts provide additional insight 
into the anomalies observed during data cleansing. Upon running 
the \texttt{jds\_datacleansings.R} program, there will be 81 charts in .PDF format in 
the \texttt{charts} directory and a \texttt{field\_usage\_summary\_table.csv}
file in the \texttt{analytics} directory. 


For validation purposes, a reference copy of the two console output files is
available on Github. These reference files can also be used to view the full console
output of the two programs, without needing to run the .R programs. Unfortunately,
the full set of 81 charts can only be duplicated by running the 
\texttt{jds\_datacleansings.R} program, as it is not possible to include all charts in either 
the main manuscript nor in this supplementary document. 



\subsubsection*{Steps for Reproducing the Analysis}
Source files are available at:
\vspace{0.1cm}

\begin{itemize}
  \item \textsc{R} source code (GitHub): \url{https://github.com/tusseyd/nyc_311_data_cleaning}
  \item USPS Zip Code dataset (Figshare): \url{https://figshare.com/s/8aea027d06f4903f2227}
  \item NYC 311 dataset (Figshare): \url{https://figshare.com/s/e8d479c391edb7224bfa}
  \vspace{0.15cm} 
  \item Data preparation console output: \\ \small\url{https://github.com/tusseyd/nyc_311_data_cleaning/blob/main/console_output/reference_JDS_data_prep_console_output.txt}
\vspace{0.15cm}
  \item Data cleaning console output: \\ \small\url{https://github.com/tusseyd/nyc_311_data_cleaning/ main/console_output/reference_JDS_datacleaning_console_output.txt}
\end{itemize}
\vspace{0.1cm}
Step 1: Download the two data Files
\begin{enumerate}
\item Download the NYC 311 and USPS Zip Code datasets from Figshare. (links above)
\item Do not rename these files as the scripts expect original filenames:
\vspace{0.1cm}
\begin{itemize}
\item \texttt{5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.zip}
\item \texttt{zip\_code\_database.csv}
\end{itemize}
\vspace{0.1cm}
\item Unzip the zip file ($\sim$9 GB uncompressed) to create a .CSV file. This .CSV 
file should be placed into the raw\_data directory. (See directory structure below.)
\item The zip\_code.csv file should also be placed into the raw\_data directory. Both
of these .CSV files will be modified by the data prep program to the .RDS 
format for ease in processing.
\end{enumerate}
\vspace{0.1 cm}
Step 2: Set Up Project Structure
\begin{enumerate}
\item Clone or download the entire GitHub repository: \\
\url{https://github.com/tusseyd/nyc_311_data_cleaning}
\item Important: Ensure you download all files from the Github repo including:

\vspace{0.1 cm}
\begin{itemize}
\item \texttt{code/data\_prep\_for\_jds\_datacleansing.R}
\vspace{0.1 cm}
\item \texttt{code/jds\_datacleansing.R}
\vspace{0.1 cm}
\item All files in \texttt{code/functions/} directory (required dependencies)
\end{itemize}
\vspace{0.1 cm}

\item Running the data preparation program \texttt{data\_prep\_for\_jds\_datacleansing.R} 
will automatically create the following directory structure. The individual files shown 
below will be created by running the data cleansing program. Below is the 
directory structure and the files they will contain after running the data cleansing program. 
\texttt{code/jds\_datacleansing.R} 
\vspace{0.1cm}
\dirtree{%
.1 nyc\_311\_data\_cleaning/ (R Working Directory).
.2 analytics/.
.3 field\_usage\_summary\_table.csv.
.2 charts/.
.3 [81 PDF files].
.2 code/.
.3 data\_prep\_for\_jds\_datacleansing.R.
.3 jds\_datacleansing.R.
.3 functions/.
.4 [function files].
.2 console\_output/.
.3 JDS\_data\_prep\_console\_output.txt.
.3 JDS\_datacleansing\_console\_output.txt.
.2 data/.
.3 5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.rds.
.3 USPS\_zipcodes.rds.
.3 raw\_data/.
.4 5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.csv.
.4 zip\_code\_database.csv.
.2 reference\_outputs/.
.3 JDS\_data\_prep\_console\_output.txt.
.3 JDS\_datacleansing\_console\_output.txt.
}
\vspace{0.1cm}
Step 3:  Open RStudio and open \texttt{code/data\_prep\_for\_jds\_datacleansing.R}
Modify the \texttt{base\_dir} variable (line 31) to match your local path:
\begin{verbatim}
base_dir <- file.path("your", "path", "here", "nyc_311_data_cleaning")
\end{verbatim}
\textit{Note: Non-Windows users should use forward slashes or platform-appropriate path separators}

\vspace{0.1cm}
Step 4: Place the downloaded data files in the following directories:
\vspace{0.1cm}
\begin{itemize}
\item \texttt{5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.csv} $\rightarrow$ \texttt{data/raw\_data/}
\item \texttt{zip\_code\_database.csv} $\rightarrow$ \texttt{data/raw\_data/}
\end{itemize}
\end{enumerate}
\vspace{0.1cm}
Step 5: Run the Data Preparation R program
\begin{enumerate}
\item Execute \texttt{data\_prep\_for\_jds\_datacleansing.R} in RStudio
\item The script will create the directory structure (if missing) and install required R packages.
\item Expected runtime: Approximately 90 minutes on a system with 16 GB 
RAM; ($\sim$30 min on a system with 32 GB RAM \textit{Note: Initial data loading is memory-intensive; the script optimizes 
memory usage after initial processing}
\item Upon completion, verify these files exist:
\vspace{0.1cm}
\begin{itemize}
\item \texttt{data/5-year\_311SR\_01-01-2020\_thru\_12-31-2024\_AS\_OF\_10-10-2025.rds}
\item \texttt{data/USPS\_zipcodes.rds}
\item \texttt{console\_output/JDS\_data\_prep\_console\_output.txt}
\end{itemize}
\end{enumerate}
\vspace{0.1cm}
Step 6: Run the Data Cleansing Analysis R program
\begin{enumerate}
\item Execute \texttt{code/jds\_datacleansing.R} in RStudio
\item Expected runtime: Approximately 60 minutes on a system with 16 GB 
RAM; $\sim$31 minutes on a system with 32 GB.
\item Monitor progress in RStudio's plot pane as charts are generated.
\item The script will produce:

\vspace{0.1cm}
\begin{itemize}
\item 81 PDF charts in \texttt{charts/}
\item \texttt{field\_usage\_summary\_table.csv} in \texttt{data/analytical\_files/}
\item \texttt{console\_output/JDS\_datacleaning\_console\_output.txt}
\end{itemize}
\end{enumerate}

\subsection*{Validation}
Compare your console output files against the reference outputs provided in the 
GitHub repository to verify successful reproduction. The console outputs contain 
detailed execution logs, row counts, and session information.

\subsection*{Notes}
\begin{itemize}
\item Total processing time: Approximately 2.5 hours on a system with 16 GB RAM; 
faster on systems with 32 GB.
\item The scripts use \texttt{data.table} for memory-efficient processing of 15+ million records
\item All file paths are configured relative to \texttt{base\_dir} for portability
\item Memory management: Initial data loading requires substantial RAM; the script 
reduces memory footprint after preprocessing.
\end{itemize}


\subsection*{Validation}
Compare your console output files against the reference outputs provided 
in the GitHub repository to verify successful reproduction. The console outputs contain
tables of information which should match your run. Validation files are located at:


\vspace{0.2 cm}
\begin{itemize}
\item Data preparation: \\ \small\url{https://github.com/tusseyd/nyc_311_data_cleaning/blob/main/console_output/reference_JDS_data_prep_console_output.txt}
\vspace{0.15cm}
\item Data cleaning: \\ \small\url{https://github.com/tusseyd/nyc_311_data_cleaning/blob/main/console_output/reference_JDS_datacleaning_console_output.txt}
\end{itemize}



\end{document}

\documentclass[linenumber]{jdsart}
\usepackage{setspace}
 
\volume{0}
\issue{0}
\pubyear{2022}
\articletype{research-article}
\doi{0000}

\usepackage{siunitx} % For alignment of numbers
\sisetup{
    group-separator = {,},
    round-mode = places,
    round-precision = 2,
    output-decimal-marker = {.},
    table-number-alignment = center,
    table-figures-integer = 6,
    table-figures-decimal = 2,
    table-figures-uncertainty = 2
}

% image path
\graphicspath{{.}{./images}}

\usepackage{xcolor}
\newcommand{\dt}[1]{\textcolor{purple}{DT: (#1)}}
\newcommand{\jy}[1]{\textcolor{red}{JY: (#1)}}

\let\proglang=\textsf
%% \newcommand{\pkg}[1]{{\fontseries{m}\selectfont #1}}
%% \newcommand\code[2][black]{\textcolor{#1}{\texttt{#2}}}

\usepackage{comment}
\usepackage{multicol}
\usepackage{fontawesome5}
\usepackage{booktabs, textgreek}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{tikz} % For creating diagrams
\usepackage{hyperref}   % For clickable links and breaking long URLs
\usetikzlibrary{positioning} % Required for relative positioning (e.g., "of" keyword)

%% float control
\renewcommand\floatpagefraction{0.75}
% \renewcommand\topfraction{.8}
% \renewcommand\bottomfraction{.8}
% \renewcommand\textfraction{.2}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}



\hyphenpenalty=950
\begin{document}

% \doublespacing 

% \tableofcontents % Optional: Table of Contents
% \listoffigures % List of Figures
% \listoftables % List of Tables

\begin{frontmatter}
  
\title{Principles for Open Data Curation: A Case Study with the New
York City 311 Service Request Data}
\runtitle{Principles for Open Data Curation}

\author[1]{\inits{D.}\fnms{David}~\snm{Tussey}}
\author[2]{\inits{J.}\fnms{Jun}~\snm{Yan}}
\address[1]{\institution{NYC Department of Information Technology and Telecommunications}, \cny{USA}}
\address[2]{Department of Statistics,
  \institution{University of Connecticut}, \cny{USA}}


\begin{abstract}
In the early 21st century, the open data movement began to transform 
societies and governments by promoting transparency,
innovation, and public engagement. The City of New York (NYC) has been at
the forefront of this movement since the enactment of the Open 
Data Law in 2012, which led to the creation of the NYC Open Data
portal. The portal currently hosts 3000 datasets,
serving as a crucial resource for research across domains such as 
health, urban development, and transportation. However, the
effective use of open data relies heavily on data quality and
usability, challenges that remain insufficiently addressed in the
literature. This paper examines these challenges via a
case study of the NYC 311 Service Request dataset, identifying key
issues in data validity, consistency, and curation efficiency. We
propose a set of data curation principles, tailored for
government-released open data, to address these challenges.
Our findings highlight the importance of harmonized field definitions,
streamlined storage, and automated quality checks, offering practical
guidelines for improving the reliability and utility of open datasets.
\end{abstract}

\begin{keywords} % no repeating those in title
  \kwd{Data cleansing}
  \kwd{Data quality}
  \kwd{Data science}
  \kwd{Data Validation}
  \kwd{NYC Open Data}
\end{keywords}

\end{frontmatter}

%-------------------------------------------------------------------------------
%	Section: Introduction
%-------------------------------------------------------------------------------

\section{Introduction} 
\label{sec:intro}

In the early 21st century, the open data movement began 
to take shape, driven by the fundamental belief that 
freely accessible data can transform both societies and 
governments. This movement champions the principles
of transparency, innovation, and public engagement. 
A landmark in this journey was the launch of the United States'
Data.gov portal in 2009 \citep{dataGov}, a pioneering
platform in making government data widely accessible. Shortly afterwards,
the European Union followed suit, unveiling its Open Data portal 
\citep{dataEU} in 2012, further cementing the movement's 
global reach. Furthermore, the World Bank's Open
Data initiative, initiated in 2010, stands out as a comprehensive
repository for global development data, available at
World Bank Open Data \citep{dataWorldBank}. 
These initiatives represent significant strides in democratizing data, 
removing barriers that once kept valuable information 
regarding government performance in silos. Their collective impact 
extends beyond mere data sharing to fostering a culture of openness 
that benefits individuals, communities, governments, and economies worldwide 
\citep{barns2016mine, wang2016adoption}.
Despite these global efforts, the effective use of open data hinges on
addressing critical data curation challenges, such as ensuring
quality, consistency, and usability.


The City of New York (NYC) has emerged as a leader in the open data movement,
marked by the enactment of the Open Data Law in 2012
\citep{zuiderwijk2014open}. This landmark legislation led to the
creation of the NYC Open Data portal \citep{dataNYC}, which 
today hosts an impressive array of 2,700 datasets
across 80 different city agencies. This wealth of data serves as a
powerful tool for researchers and policymakers, significantly
enhancing local government transparency.
Popular datasets include information on
restaurant health inspection violations, car crashes, high school and
college enrollment statistics, jail inmate charges, and the location
of citywide free Internet access points. These datasets have been
applied in civil life in various ways, such as mapping car crashes
involving pedestrians and visualizing high school and college
enrollment trends. Furthermore, they have enabled significant research
across multiple domains, including health \citep{cantor2018facets, 
shankar2021data}, urban development \citep{neves2020impacts}, and
transportation \citep{gerte2019understanding}, aiding in the
understanding and addressing of complex urban challenges.
For example, NYC's 311 data has been used to optimize resource
allocation in urban planning and improve emergency response times,
showcasing its practical relevance in real-time decision-making.


However, open data pose substantial curation challenges.
Data curation, the process of organizing, maintaining, and ensuring
the quality of datasets, plays a crucial role in maximizing the
utility of open data. Proper curation ensures that datasets remain
consistent, accurate, and useful for diverse applications. For example, 
well-curated data are essential for machine learning systems, which 
require high-quality data to produce reliable insights 
\citep{polyzotis2019data, jain2020overview}. A critical component of 
data curation is data cleaning, which involves identifying and rectifying 
inconsistencies, errors, and inaccuracies in datasets. While open data 
initiatives have made vast amounts of data available, ensuring the 
reliability and utility of such data hinges on rigorous data cleaning processes. 
Efficient usage of software tools can significantly streamline this 
aspect of curation \citep[e.g.,][]{cody2017cody, van2018statistical}. Poor 
curation may result in issues such as missing data, formatting errors, 
or inconsistent values, leading to biased or inaccurate outcomes 
\citep{geiger2020garbage}. This is particularly critical in domains 
where machine learning is applied to sensitive tasks, such as public 
health or policy \citep{rahm2000data}. 


Research into data curation has explored these challenges in-depth. 
Among the earliest discussions, \citet{witt2009constructing} focused 
on developing data curation profiles tailored to specific contexts, 
setting a precedent for targeted data management strategies. 
Addressing broader challenges in data sharing and management, 
\citet{borgman2012conundrum} highlighted the complexities of 
research data distribution, emphasizing the need for robust 
strategies. This is complemented by \citet{hart2016ten}, who outlined 
essential principles for effective data management, particularly 
emphasizing meticulous curation practices. The utility of curated 
open data is vividly illustrated in public health and global challenges, 
where \citet{cantor2018facets} demonstrated the utility of curated 
data in evaluating community health determinants, and 
\citet{shankar2021data} observed its critical role during the 
COVID-19 pandemic in managing collective responses.
Despite these advances, there remains a notable gap in providing
actionable frameworks for managing large-scale municipal datasets.


By addressing this gap, this study transitions from theory to
practical insights, aiming to establish principles that enhance both
the reliability and utility of open government data.
The contributions of this paper are twofold. First, we delve into
the specifics of data curation challenges using the NYC 311 Service
Request (SR) data as a case study. This dataset serves as a prime 
example for examining key issues in data curation, including data 
validity, consistency, and curation efficiency. We illustrate these 
points with examples drawn from our 
processing of the 311 SR data. Second, we propose a set of data curation 
principles tailored for government-released open data. These 
principles are designed to address the unique challenges 
and requirements observed in the curation of such datasets.


The paper is organized as follows. Section~\ref{sec:data} provides 
an overview of the NYC 311 SR system and presents 
a summary of SR counts over a five-year period (2020---2024). 
Section~\ref{sec:issues} delves into specific data cleansing 
challenges affecting data quality including structural issues, 
adherence to the data dictionary, and the presence of missing entries. 

This section also investigates data field compliance with reference or acceptable values, 
highlights logical inconsistencies, and examines some concerning patterns 
in the data. We explore the balance between precision and accuracy 
and identify duplicate or redundant fields, along with observations 
on the Data Dictionary \citep{datadictionaryNYC}.

Section~\ref{sec:recommendations} offers 
practical recommendations for mitigating or resolving these issues, 
while Section~\ref{sec:conclusion} encapsulates key insights and 
discusses the broader implications of our findings.

%-------------------------------------------------------------------------------
% Section: NYC 311 Service Request (SR) Data
%------------------------------------------------------------------------------

\section{NYC 311 Service Request (SR) Data} 
\label{sec:data}

The NYC 311 service, a critical component of New York City's public
engagement and service response framework, serves as a centralized hub
for non-emergency inquiries and requests. Introduced in 2003, the NYC
311 system was designed to streamline the city's response to
non-emergency issues, ranging from noise complaints to street
maintenance requests. Initially a phone-based call center, the system
evolved into a comprehensive data management platform handling
millions of requests annually. Key milestones since its launch in 2003
include the addition of online and mobile channels in 2009, a
record monthly high of 348,463 SRs in 
August 2020 during the COVID-19 
pandemic, the 2021 expansion to include the 
Metropolitan Transit Authority (MTA) NYC subway 
system (the biggest expansion in 311 history), 
and a yearly record of 3.5 million SRs in 2024. 
Figure~\ref{fig:annualSRs} illustrates the nearly 18\% growth since 2020.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{annual_trend_with_projection_bar_chart.pdf}
  \caption{Annual SR Volume with 2025 Projection}
  \label{fig:annualSRs}
\end{figure}


Today, the NYC 311 data system manages over 3 million Service
Requests (SRs) per year. The data are publicly accessible 
through the NYC Open Data portal, which provides 
tools for querying, aggregating,
visualizing, and exporting results. The 
NYC Office of Technology and Innovation (OTI) 
provides technical support for the 311 application, 
the open data infrastructure, mobile, and web applications, 
and the public-facing data portal. Input data for the 311 system 
are sourced from 16 different NYC agencies. Some of these agencies
use the core 311 software, while others transfer data from their 
in-house systems to the 311 Open Data 
Warehouse.


The impact of NYC 311 data extends beyond operational efficiency; it
has become instrumental in shaping city governance and community
engagement. Open data not only ensure governmental transparency
but also empower civic developers, the general public, and
policy-makers \citep{minkoff2016nyc, o2017uncharted,
  kontokosta2021bias}.
The data have been pivotal in providing advice on shelters
during emergencies, handling inquiries during the COVID-19 
pandemic, enforcing standards between landlords 
and tenants, reallocating taxi
routes based on analyses by the Taxi and Limousine Commission (TLC),
and improving responsiveness across city agencies.
It also supports a wide range of studies in resource allocation and
emergency response strategies \citep{zha2014profiling, raj2021swift},
social equity in service provision \citep{white2018promises,
  kontokosta2021bias}, and urban challenges such as noise pollution
\citep{dove2022sounds} and street flooding
\citep{agonafir2022understanding}.


Our investigation uses a 311 SR dataset covering a five-year period (2020---2024), 
collected on 10 October 2025. This dataset is used 
to conduct analysis in data consistency, data validity, 
redundant fields, etc. Instructions 
on downloading this file from the NYC Open Data portal 
are available in the supplemental material. 
Characteristics of this five-year dataset include:

\begin{itemize}[left=1.5em, itemsep=0.25\baselineskip]
\item The dataset is approximately 9 GB in size and contains 
over 16 million rows, each row representing a single 
SR. The dataset exports as a CSV file. For efficiency in the R 
programming environment, the file was converted to RDS format.

\item Each record has 41 columns (fields) of data, representing 
a single SR (complaint, issue, request). 

\item There are 252 distinct \texttt{complaint\_type}(s). The top twenty
account for 68\% of all SRs. 
	
\item Each SR has four date fields in the YYYY-MM-DD HH:MM:SS format: 

\begin{multicols}{2}
\begin{itemize}[label=\textbullet]
  \item \texttt{created\_date}
  \item \texttt{closed\_date}
  \item \texttt{resolution\_action\_updated\_date}
  \item \texttt{due\_date}
\end{itemize}
\end{multicols}

\item There are two borough fields, \texttt{borough} and \texttt{park\_borough}, 
		which appear to be duplicates.
  
\item There are seven street-related fields, two pairs of which appear to be duplicates:

\begin{multicols}{2}
\begin{itemize}[label=\textbullet]
    \item \texttt{incident\_address}
    \item \texttt{street\_name}
    \item \texttt{cross\_street\_1}
    \item \texttt{cross\_street\_2}
    \item \texttt{intersection\_street\_1}
    \item \texttt{intersection\_street\_2}
    \item \texttt{landmark}
\end{itemize}
\end{multicols}

          
\item In addition to \texttt{incident\_address}, each SR has six
	additional geographic fields:

\begin{multicols}{2}
\begin{itemize}[label=\textbullet]
    \item \texttt{latitude} \& \texttt{longitude}
    \item \texttt{location}
    \item \texttt{street\_name}
    \item \texttt{landmark}
    \item \texttt{block}
    \item \texttt{x \& y\_coordinate\_state\_plane}
\end{itemize}
\end{multicols}

	
\item A free-form text field, \texttt{resolution\_description}, which 
supports 930 characters including commas and special 
characters; often problematic for automated processing.
\end{itemize}

Noise-related complaints are especially prominent, represented by eight separate
categories (e.g., vehicle, residential, helicopter, street/sidewalk). 
Collectively, these noise complaints account for nearly one-quarter (23.4\%)
of all complaints, making \textit{noise} by far the most common complaint.
Other major categories include illegal parking, heat/hot water, and blocked
driveway.

In addition to noise, prominent categories 
include illegal parking, heat/hot water 
complaints, and blocked driveways. Figure~\ref{fig:SR_complaints}
reveals that the Top 20 \texttt{complaint\_type(s)} comprise 68\% of all SRs,
with the remaining 190 \texttt{complaint\_type(s)} spread across
a thin tail. This suggests that improving 
responses to the most frequent complaints could have an 
outsized impact on overall service efficiency. 

\begin{figure}[tbp]
 \centering
  \includegraphics[width = \textwidth]{SR_by_complaint_type_pareto_combo_chart.pdf} 
  \caption{Top 20 complaint types (2020---2024)} 
  \label{fig:SR_complaints}
\end{figure}

Dataset anomalies are typically associated with specific city
agencies, which can facilitate targeted corrective actions. For example,
some anomalies occur in only one agency, enabling precise remediation.
Figure~\ref{fig:SRcountbyAgency} provides a breakdown of SRs by
the responsible agency, showing the cumulative 
percentage of SRs handled by each
agency over the five-year time frame. Note that the
distribution of SRs is heavily concentrated among a few key 
agencies with the six largest agencies 
being the New York Police Department (NYPD), 
Department of Housing Preservation and Development (HPD), 
Department of Sanitation (DSNY), Department of Transportation (DOT), 
and Department of Environmental Protection (DEP) which 
collectively handle 90\% of all SRs. The 
remaining 10\% of SRs are distributed across 10 additional 
agencies. This concentration of SRs to these ``big six'' agencies 
underscores the necessity for optimal data curation processes 
within these agencies. 

\begin{figure}[tbp]
  \centering
  \includegraphics[width=\textwidth]{SR_by_agency_pareto_combo_chart.pdf}
  \caption{Distribution of SRs by NYC agency (2020--2024)}
  \label{fig:SRcountbyAgency}
\end{figure}

%-------------------------------------------------------------------------------
% Section: Data Cleansing Issues and Anomalies
%-------------------------------------------------------------------------------

\section{Data Cleansing Issues} 
\label{sec:issues}
Data cleansing refers to the process of identifying and rectifying
errors, inconsistencies, and inaccuracies within datasets to ensure
they are of high quality and reliable for analysis
\citep{maletic2005data, hosseinzadeh2023data}. The process
typically involves removing duplicate records, handling missing or
incomplete data, correcting mislabeled or inaccurate entries, and
standardizing data formats \citep[e.g.,][]{cody2017cody, van2018statistical}. In 
the context of open data, cleansing is especially important 
as open datasets often come from diverse,
often uncoordinated sources, leading to variations in data quality,
completeness, and consistency. Without thorough data cleansing, 
the utility of open data can be limited and perhaps 
untrustworthy, affecting its reliability for research, 
policy making, and innovation. The main purpose of cleansing 
open data is to ensure that it is accurate, consistent, and usable 
across multiple analytical platforms and by various stakeholders with
myriad purposes. Data cleansing improves the trustworthiness 
of the data and enables more accurate analysis, 
better decision making, and the improved integration of 
data into machine learning models or other systems.


Many quality criteria are employed to ensure high-quality data 
processing. One of the primary efforts is data validation, which includes 
several critical checks. For instance, mandatory fields must not be 
left empty, ensuring that key information is always captured. 
Additionally, certain fields must conform to specific data types, 
such as numeric, character, or date formats; typically 
outlined in a Data Dictionary. An essential aspect of 
validation is domain compliance, where 
fields must adhere to a predefined set of 
values such as statuses, state names, or zip codes. Structural 
errors can also play a significant role in data quality, particularly when 
naming conventions or data entries are inconsistent. Common issues 
include fields that do not appear in the Data Dictionary and  
inconsistent usage of data fields. Furthermore, redundant 
(or irrelevant) fields can clutter datasets, reducing efficiency, 
creating confusion, and introducing errors. Logical inconsistencies 
are another consideration, such as related fields that violate expected 
relationships, for example a \texttt{due\_date} that 
precedes \texttt{created\_date}.
Lastly, the balance between accuracy and precision is crucial, as both 
must be managed to ensure meaningful data.


We identify the presence of issues in the NYC 311 SR open dataset;
not attempting to solve them, but rather to highlight their scope, 
magnitude, and potential impact. We would recommend 
that any corrective solution should be undertaken only 
after further investigation as to the why and how such 
issues might come about, including  detailed discussions 
with the originating agency as to whether or not the issue is an 
actual error, correct, or holds some other status.

%-------------------------------------------------------------------------------
% Sub-section: Pre-analysis Data Modifications
%-------------------------------------------------------------------------------

\subsection{Data Preprocessing and Standardization}
\label{subsec:premodifications}
Prior to analysis, we subject the raw dataset to a series of preparatory
modifications to facilitate efficient processing. These steps include both
structural adjustments and content standardization:

\begin{itemize}[left=1.5em, itemsep=0.25\baselineskip]
  \item Standardization of structure: Field names are normalized for ease of reference, and the raw
  \texttt{.csv} file is converted into the more efficient \texttt{.rds} format.

  \item Date handling: All date fields are converted to the \texttt{POSIXct} class and aligned
  to the \texttt{America/New\_York} time zone, enabling accurate 
  treatment of Daylight Saving Time (DST) effects (discussed in
  Section~\ref{par:dst}).

  \item Agency harmonization: Several agencies were renamed during the study period; records were
  consolidated under their current agency designations.

  \item Validation of completeness: Missing values—recorded in multiple 
  formats—are standardized to \texttt{NA} to facilitate processing. 

  \item Field formatting: Text fields are converted to uppercase for 
  consistent matching, and numeric fields are explicitly cast to the appropriate type.
\end{itemize}

%-------------------------------------------------------------------------------
% Sub-section: Structural Issues
%-------------------------------------------------------------------------------

\subsection{Structural Issues}
\label{sec:structural}
Structural issues refer to how data are organized, formatted, 
or presented within a dataset. They can make 
it difficult to analyze the data effectively and often require extensive 
data cleaning efforts. We encountered three structural issues:
precision versus accuracy issues in the 
\texttt{latitude}, \texttt{longitude}, and \texttt{location} 
fields, inconsistent representations of missing data, 
and systemic daylight saving time anomalies.

\paragraph{Precision versus Accuracy in Latitude and Longitude Values:}
\label{par:precision}
The \texttt{latitude}, \texttt{longitude}, and the duplicative \texttt{location}
fields in the raw dataset exhibit substantial variation in numeric precision.
The vast majority (90\%) of values 
are recorded with thirteen decimal places, 
(e.g., \texttt{latitude} = 40.7022221884127, \texttt{longitude} = -73.9126848719299).
However, the range of precision spans from eight to fifteen decimals for 
\texttt{latitude} and from zero to fifteen for \texttt{longitude}. A small 
subset of 212 records contains only integer longitude values
with no decimals. Moreover, positional precision at thirteen decimal places
implies a spatial resolution at the molecular scale—certainly implausible.
This is a classic case of excessive precision rather than meaningful accuracy. 
This span of precision could suggest inconsistent data capture. It is
highly likely that these coordinate fields are the result of a geocoding of the
SR \texttt{incident\_address}, especially since 90\% of the values share
that same level of precision. It would be useful to evaluate the functionality
of the geocoding software. 

For reference, the official New York City Borough Boundary--Water Included
 (\textsc{NYBBWI}) dataset, published by the Department of City Planning (DCP),
provides coordinates to six decimal places, corresponding to a practical
accuracy of a few meters. In this analysis, the original
coordinates were retained without rounding, but were evaluated against the
official NYC city boundary contained in (\textsc{NYBBWI}), but using a 
tolerance of \SI[round-precision=0]{100}{\meter}. All met that criterion.
 
\paragraph{Inconsistent Representation of Missing data:} 
We found that the 311 SR dataset exhibited 
inconsistent approaches to 
presenting missing data. We discovered multiple different 
representations of missing data: nulls, spaces, ``\texttt{NA}'', ``\texttt{N/A}'', 
and ``\texttt{<NA>}''. This variety of representations 
complicates programming efforts and 
increases the likelihood of introducing errors 
typically by searching for or excluding 
missing fields, and not being aware of the various 
representations of such. A more consistent 
approach to treating missing data should 
be employed. To address this issue, 
all missing values were standardized to \texttt{NA}. Notably, 
\SI[round-precision = 2]{27.86}{\percent}
 of the dataset contains missing entries.

\paragraph{Anomalies Associated with Daylight Saving Time (DST):}
\label{par:dstsystemic}
 The 311 SR dataset captures local NYC times 
 (e.g., \texttt{America/New\_York}). 
Accordingly, twice each year there are changes to 
the local time as a result of observing 
DST. This can introduce some unexpected results. For example, 
on both March 13, 2022, and March 12, 2023, there are no times 
between 1:59AM and 2:59AM. The 2:00AM hour on those dates 
is simply missing owing to the clocks moving forward; at 2:00AM 
it instantly becomes 3:00AM. This creates a gap in the hourly 
distribution of SRs on those days, and exaggerates the 
lifespan of SRs, albeit by a single hour. 

During preprocessing, several date fields were found to violate the DST
spring-forward rule, with timestamps recorded during the nonexistent hour
between 02:00:00 and 02:59:59
(e.g., \texttt{2024-03-10 02:00:46},
\texttt{2020-03-08 02:10:00}). Such values are invalid, as local time
advances directly from 01:59:59 to 03:00:00 on those dates. These anomalies
were corrected by shifting the affected timestamps forward one hour 
to valid times (e.g., \texttt{2024-03-10 03:00:46},
\texttt{2020-03-08 03:10:00}). Nineteen such corrections were made.

The fall DST adjustment introduces a more serious systemic issue by producing
SRs with \emph{negative} durations, where the \texttt{closed\_date} precedes
the \texttt{created\_date}. Such cases are logically impossible and can
distort agency-level performance measures, particularly for time-sensitive SRs.
The following example illustrates the problem:


\begin{itemize}[left=1.5em, itemsep=0.25\baselineskip]
  \item A residential noise complaint is received by 311 at 01{:}50 on Sunday,
  2024-11-03, the date of the DST fallback transition. An SR is created.

  \item The NYPD responds promptly, resolving the issue in forty minutes.  
  Without DST adjustment, the resolution would be recorded at
  02{:}30.

  \item However, during this interval the local clock is set back one hour, 
  so the resolution is time-stamped in the 311 system as 01{:}30 local time. 

  \item As a result, the SR is recorded as having closed twenty 
  minutes \emph{before} it was created, yielding a nonsensical negative 
  duration of -20~minutes.
\end{itemize}


On average, 86 SRs per year exhibit this systemic issue, 
with a mean negative duration of \SI[round-precision=2]{-42.24}{minutes}. 
Although rare, these anomalies can disproportionately affect 
time-sensitive complaint categories such as
\texttt{drug\_activity}, \texttt{homeless\_assistance}, and
\texttt{disorderly\_youth}, and they can distort aggregate performance metrics.
Even a small number of implausible durations can shift summary statistics and
obscure true operational patterns. Additional analysis of 
DST effects are found in Section~\ref{sec:dst}.


%-------------------------------------------------------------------------------
% Sub-section: Missing Data by Field
%-------------------------------------------------------------------------------

\paragraph{Missing Data by Field:}
\label{sec:blanks}
Understanding the absence of data by field greatly aids analysis. 
For example, determining if SRs were closed 
before or after their \texttt{due\_date}, it would be challenging 
as 99.6\% of these entries are empty. We found 
that field completeness divided into three groups:

\begin{itemize}[left=1.5em, itemsep=0.25\baselineskip]
    \item Complete: 94--99\% filled.
    \item Moderately Complete: 57--88\% filled;
    \item Sparse/Incomplete: 0.05--1.5\% filled
\end{itemize}

Table~\ref{tab:completeness-lower} summarizes fields exhibiting moderate or
low completeness rates.

\begin{table}[tbp]
\centering
\caption{Fields with moderate or sparse data completeness (2020--2024)}
\label{tab:completeness-lower}
\small
\begin{tabular}{l r r}
\toprule
\multicolumn{1}{c}{Field} & 
\multicolumn{1}{c}{Filled count} & 
\multicolumn{1}{c}{Completeness (\%)} \\
\midrule
\texttt{taxi\_company\_borough} & \num[round-mode=none]{8564} & 0.05 \\
\texttt{due\_date} & \num[round-mode=none]{52346} & 0.33 \\
\texttt{vehicle\_type} & \num[round-mode=none]{181533} & 1.13 \\
\texttt{facility\_type} & \num[round-mode=none]{211240} & 1.32 \\
\cmidrule(lr){1-3}
\texttt{landmark} & \num[round-mode=none]{9058628} & 56.56 \\
\texttt{intersection\_street\_1} & \num[round-mode=none]{10251224} & 64.00 \\
\texttt{intersection\_street\_2} & \num[round-mode=none]{10257371} & 64.04 \\
\texttt{cross\_street\_2} & \num[round-mode=none]{11693685} & 73.01 \\
\texttt{cross\_street\_1} & \num[round-mode=none]{11696472} & 73.03 \\
\texttt{address\_type} & \num[round-mode=none]{13246948} & 82.71 \\
\texttt{location\_type} & \num[round-mode=none]{13823312} & 86.31 \\
\texttt{bbl} & \num[round-mode=none]{14128219} & 88.21 \\
\bottomrule
\end{tabular}
\end{table}

For some analysis efforts, it is suggested to inquire 
as to why some fields are almost always blank. Additionally, 
field usage varies considerably across agencies. For instance,
\texttt{landmark} and \texttt{facility\_type} are widely used across agencies,
underscoring their importance in classifying SRs. By contrast,
\texttt{taxi\_pick\_up\_location} is used exclusively by TLC. 
Table~\ref{tab:field-usage-by-agency} highlights the varied usage of
certain fields across NYC agencies, revealing both specialization and
redundancy. Fortunately, most of the fields used in 
this analysis are well populated.

\begin{table}[tbp]
\centering
\caption{Sample field usage by select agencies (2020--2024)}
\label{tab:field-usage-by-agency}
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{
  l
  S[table-format=7.0,round-precision=0]
  S[table-format=7.0,round-precision=0]
  S[table-format=7.0,round-precision=0]
  S[table-format=7.0,round-precision=0]
  S[table-format=7.0,round-precision=0]
  S[table-format=7.0,round-precision=0]
  S[table-format=7.0,round-precision=0]
}
\toprule
\multicolumn{1}{c}{Field} &
\multicolumn{1}{c}{DEP} &
\multicolumn{1}{c}{DHS} &
\multicolumn{1}{c}{DOB} &
\multicolumn{1}{c}{DOT} &
\multicolumn{1}{c}{DPR} &
\multicolumn{1}{c}{DSNY} &
\multicolumn{1}{c}{TLC} \\
\midrule
\texttt{landmark}                 & 2935   & 70185  & 282   & 135150  & 254218  & 424284  & 63133  \\
\texttt{facility\_type}           & 175549 & 0      & 51575 & 159667  & 0       & 66012   & 0      \\
\texttt{due\_date}                & 0      & 0      & 0     & 0       & 0       & 32097   & 0      \\
\texttt{vehicle\_type}            & 0      & 0      & 0     & 0       & 0       & 0       & 1696   \\
\texttt{taxi\_company\_borough}   & 0      & 0      & 0     & 0       & 0       & 0       & 4025   \\
\texttt{taxi\_pick\_up\_location} & 0      & 0      & 0     & 0       & 0       & 0       & 72558  \\
\texttt{bridge\_highway\_name}    & 15     & 21012  & 0     & 7451    & 0       & 613     & 977    \\
\texttt{road\_ramp}               & 15     & 2588   & 0     & 7450    & 0       & 613     & 999    \\
\bottomrule
\end{tabular}%
}
\end{table}


%-------------------------------------------------------------------------------
% Sub-section: Validating Data for Compliance with Allowable Values
%-------------------------------------------------------------------------------

\subsection{Validating Data for Acceptable Values}
\label{sec:domain}
Any analytic effort must ensure that fields containing valid values
as identified by a reference source. Invalid entries should 
be identified and isolated from the analysis. For example, the \texttt{latitude} 
and \texttt{longitude} fields were found to all fall within the 
geographic boundaries of New York City. For some attributes, 
the Data Dictionary specifies domains. For others, the documented 
domain was incomplete or inaccurate, and valid values had 
to be inferred from operational usage, supplemental
documentation, or external datasets. Additonally, the NYC Open Data 
portal supplements the Data Dictionary by by indicating 
field types such as text, numeric, date, and geospatial.
We discovered several anomalies where 
observed legal values did not align with published specifications.

The following fields were tested and found to comply with their expected
domains, as determined by public usage and as evidenced in 
other historical datasets:

\begin{multicols}{2}
\begin{itemize}[left=1.5em, itemsep=0.1\baselineskip]
  \item \texttt{address\_type}
  \item \texttt{status}
  \item \texttt{borough}
  \item \texttt{borough\_boundaries}
  \item \texttt{park\_borough}
  \item \texttt{data\_channel}
  \item \texttt{vehicle\_type}
  \item \texttt{city\_council\_district}
\end{itemize}
\end{multicols}


However,  some fields proved problematic as to their 
compliance with a domain of allowable values. These include

\paragraph{ZIP Codes:}
\label{sec:zipcodesissues}
All \texttt{incident\_zip}(s) should 
validate against the United States Postal Service (USPS) database, which contains 
42,735 valid ZIP codes. We discovered that the \texttt{incident\_zip} 
field has 0.02\% invalid entries, resulting in 3274~errors. This includes
ZIP codes such as ``11111'', ``00000'', and ``00083''. The presence of 
invalid ZIP codes is notable because this field is
among the easiest to validate against an authoritative reference.

\paragraph{Community Boards:}
\label{sec:communityboardissues}
The \texttt{community\_board} field captures one of New York City’s 
local governance units. Validation should be straightforward, given a publicly
available list of community boards. Nonetheless, this analysis identified
56,914 (\SI[round-precision = 2]{0.36}{\percent}) invalid entries. 

%-------------------------------------------------------------------------------
% Sub-section: Date Field Issues
%-------------------------------------------------------------------------------

\subsection{Date Field Issues}
\label{subsec:datefieldissues}
In addition to the DST-related structural anomalies discussed in
Section~\ref{par:dst}, the dataset exhibits further temporal 
issues. The NYC~311 dataset includes four such fields:
\texttt{created\_date}, \texttt{due\_date}, \texttt{resolution\_action\_updated\_date}, and
\texttt{closed\_date}. We found these date fields often
presented implausible or extreme relationships with one another, 
complicating both analysis and interpretation.

\paragraph{Created Date:}
\label{sec:createddate}
All \texttt{created\_date} values were checked to ensure that none were set in
the future or before the inception of the 311 system (circa~2003); all records
were compliant. However, a notable clustering of timestamps occurs at exactly
midnight (14,409 instances) and exactly noon (31,872 instances), an anomaly
examined further in Section~\ref{par:midnightnoonissues}.

\paragraph{Due Date Anomalies:}
\label{par:duedate}
The \texttt{due\_date} field is used almost exclusively by \textsc{DSNY} 
and is populated in only about
\SI[round-precision = 2]{0.05}{\percent} of cases. All values passed validation
(no implausibly past or future dates), and no unusual patterns were observed.
However, sixteen cases were found where the \texttt{due\_date} precedes the
\texttt{created\_date}—a nonsensical condition. These ranged from just over
one year early to more than two years early. 

\paragraph{Closed Date:}
\label{par:closeddate}
Most records were compliant with respect to temporal bounds; none were set
before 2003 or in the future. However, there is a clustering of 
timestamps at exactly midnight 
(1,083,862; \SI[round-precision = 2]{6.77}{\percent}) 
of  \texttt{closed\_date}(s)) and exactly noon
(294,817; \SI[round-precision = 2]{1.84}{\percent}) representing
 a significant artifact, discussed further in 
 Section~\ref{par:midnightnoonissues}. Additionally, 45,296 entries
show \texttt{closed\_date}(s) preceding \texttt{created\_date}(s); 
these negative durations are examined in detail in 
Section~\ref{subsec:duration}.

\paragraph{Resolution Action Updated Date:}
\label{par:resolutionupdatedate}
The \texttt{resolution\_action\_updated\_date} field records modifications or
follow-up actions related to an SR. No values were found in implausible years,
however, 374,974 entries
(\SI[round-precision = 2]{2.34}{\percent}) occur before the corresponding
\texttt{created\_date}. 
Nearly \SI[round-precision=0]{80}{\percent} of these
premature dates occur within a day (often within seconds) 
of the \texttt{created\_date}, 
suggesting automated prepopulation during early processing.
A more consequential anomaly concerns post-closure updates.

While it is certainly possible---and even routine---to 
update an SR after it is closed, 
\SI[round-precision=0]{54}{\percent} of updates
do occur after the \texttt{closed\_date}.
However, some of these updates appear to be 
well beyond a normal or expected timeframe. We found
 27,810 entries that occur more than 30~days—and up to 1593~days (more than 
four years)—post-closure. Figure~\ref{fig:resolution-violin} illustrates
both the extent and volume of these long post-closure updates,
showing a notable concentration of updates within 
the 30--200 day range. This raises 
questions about whether such delayed updates are standard 
practice or indicative of a potential issue. Note 
that 84\% of these late updates are associated with the TLC agency
and 12\% with DSNY. Finally, 91 records exhibit extremely 
late post-closure updates due to invalid \texttt{closed\_date} 
values such as 1899-12-31 or 1900-01-01 which create
an artificially post-closure update duration. Such behavior 
could possibly be explained by the well-known 
Structured Query Language (SQL) default-date behavior 
associated with null datetimes 
(e.g., defaulting to 1900-01-01). A similar behavior can 
arise in Microsoft Excel. 

\begin{figure}[tbp]
\centering
% First figure
\begin{subfigure}[t]{0.495\textwidth}
\centering
\includegraphics[width=\textwidth]{post_closed_resolution_violin.pdf}
\caption{\textsc{SR} post-closure updates (30--1095~days)}
\label{fig:resolution-violin}
\end{subfigure}
\hfill % Horizontal space between subfigures
% Second figure
\begin{subfigure}[t]{0.495\textwidth} % Adjust width as needed
\centering
\includegraphics[width=\textwidth]{negative_duration_SR_violin.pdf}
\caption{Negative SR durations (<-1 year)}
\label{fig:negative-duration-violin}
\end{subfigure}
% Main figure caption
\caption{Violin plots for post-closure resolution action updates and negative SR durations}
\label{fig:violin-plots}
\end{figure}

\paragraph{Daylight Saving Time (DST) Anomalies:}
\label{par:dst}
As discussed in Section~\ref{par:dstsystemic}, DST adjustments create a systemic 
error. The DST \mbox{fall-back} adjustment yields implausible 
negative durations in approximately 86 SRs per year, with a 
mean negative duration of 
\SI[round-precision=2]{-42.24}{minutes}. Similarly, the spring-forward 
adjustment inflates durations for approximately 55,000 SRs 
annually due to the missing 02{:}00-–02{:}59 
interval. Although offsets of +1 hour or -42 minutes 
seem minor, such distortions can notably affect performance metrics for
time-sensitive complaints such as \texttt{Noise--Residential},
\texttt{Homeless~Person~Assistance}, and
\texttt{Drug~Activity}. 

\paragraph{Midnight and Noon: Creation and Closure Issues:}
\label{par:midnightnoonissues}
SR activity generally follows a typical workday pattern, with the majority
of requests created during daytime hours. Closer examination of 
the \texttt{created\_date} and \texttt{closed\_date}
fields reveals substantial peaks exactly at midnight (00{:}00{:}00) and noon
(12{:}00{:}00). These peaks suggest the presence of an automated 
batch-processing process, perhaps one that automatically 
assigns timestamps of midnight (00:00:00) to 
large batches of SRs prior to sending the data to the 311 
data warehouse. This behavior distorts the 
calculated duration of these 
SRs as well as overall system usage by providing inaccurate 
timestamps. Distribution by Agency shows that over 92\% of 
the ``closed-exactly-at-midnight'' SRs come from just two agencies:
Department of Buildings (DOB) (69\%) 
and DSNY (23\%).

One possible explanation for the midnight peak is that some records were
originally stored with only a YYYY-MM-DD calendar date 
(e.g., \texttt{2024-09-11}) and later converted to a datetime type 
during processing.
When a timestamp is absent and field is converted to a datetime 
value, typical system behavior is to assign a default value of
\texttt{00{:}00{:}00}. Such behavior would, of course, artificially 
inflate counts at midnight. This, however, does not explain the spikes
at noon. As shown in Figure~\ref{fig:exacthours}, both 
\texttt{created\_date} and \texttt{closed\_date} timestamps
display significant spikes at both noon and midnight, 
exceeding both random expectations and a \(+3\sigma\) threshold.

\begin{figure}[tbp]
  \centering
  \begin{subfigure}[b]{0.495\textwidth}
    \includegraphics[width=\textwidth]{created_top_of_hour_distribution.pdf}
    \caption{\textsc{SR}s created at the top of the hour}
    \label{fig:created_top_hour}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.495\textwidth}
    \includegraphics[width=\textwidth]{closed_top_of_hour_distribution.pdf}
    \caption{\textsc{SR}s closed at the top of the hour}
    \label{fig:closed_top_hour}
  \end{subfigure}
\caption{311~\textsc{SR}s created and closed exactly at the top of the hour (HH:00:00)}
  \label{fig:exacthours}
\end{figure}

%-------------------------------------------------------------------------------
% Sub-section: Duration Issues
%-------------------------------------------------------------------------------
\subsection{Duration Issues}
\label{subsec:duration}

SR Duration is one of the most frequently analyzed 311 
performance metrics, informing 
assessments of equity in service across neighborhoods and 
boroughs and serves as a proxy for agency
responsiveness. Given this, SR duration
warrants close scrutiny. While duration is not directly 
present in the dataset, 
it is easily computed as the difference between
\texttt{closed\_date} and \texttt{created\_date}.  Given the prevalence of
automated workflows, these timestamps might be expected to be populated
programmatically in response to user actions such as a change in 
SR \texttt{status}. However, this appears not to be the case, as 
the dataset includes anomalies in these fields that would ordinarily be
prevented by robust automation.

\paragraph{Negative Durations:}
\label{par:negativedurations}
A total of 45,296 SRs (0.29\%) have a \texttt{closed\_date} that precedes the
\texttt{created\_date}, producing a nonsensical negative duration. 
Figure~\ref{fig:negative-duration-violin} illustrates the range 
and volume of these negative durations with a cutoff at 3 years. 
Across all affected SRs, negative durations 
average \SI[round-precision=1]{-118.7}{\day}, a non-trivial distortion. 
Within this group, 115 SRs show \emph{extreme} negative 
durations due to default \texttt{closed\_date} values 
---1899-12-31 and 1900-01-01--- extreme negative 
durations exceeding -122~years. These extreme negative 
durations are overwhelmingly concentrated within the
DOT which accounts for approximately \SI[round-precision=0]{98}{\percent} 
of all such cases. 

\paragraph{Zero and One-Second Durations:}
A larger issue arises when \texttt{closed\_date} and 
\texttt{created\_date} have exactly the same timestamp, 
to the second. This creates a nonsensical \emph{zero duration}. 
There are 387,329 (2.5\%) such zero duration 
SRs. In addition, 3460 SRs are closed within one second of their
creation. While not logically impossible, these near-instantaneous 
SR resolutions are highly improbable. Moreover, 
\SI[round-precision = 0]{81}{\percent} of such cases 
originate from just three agencies: 
Department of Health and Mental Hygiene (DOHMH), DOT, and 
DOB.

\paragraph{Suspicious Durations:}
\label{par:suspiciousdurations}
Service request (\textsc{SR}) durations are right-skewed, with some cases
extending for months or years. An analysis was undertaken to
determine appropriate lower-bound thresholds for identifying highly
implausible or suspiciously small duration values. Durations were truncated to
$>$2~seconds (excluding zero- and one-second artifacts) and to
$\leq$172{,}800~seconds (48~hours). Five complementary outlier screening
methods were evaluated: standard deviation (SD), median absolute deviation
(MAD), log-normal thresholds, percentile cutoffs (1--10\%), and interquartile
range (IQR). SD, MAD, and IQR tests did not flag further anomalies, whereas
log-normal and percentile methods produced operationally reasonable limits.
This resulted in a log-normal $+3\sigma$ cutoff (\SI{27.95}{\second}),
which was then used as a \emph{suspicious} cutoff. This identified
\SI[round-precision=2]{0.07}{\percent} of records as outliers (14{,}741~SRs)
with suspicious durations. Combining zero durations
(387{,}329), one-second durations (1{,}240), and suspicious durations
$<$~28~seconds (14{,}471), a total of 403{,}310~SRs
(\SI{2.6}{\percent}) were judged as statistically suspicious or
operationally implausible.


\paragraph{Positive Durations:}
\label{par:positivedurations}
While a positive duration is not inherently erroneous, some SRs exhibit
extremely long lifespans. A total of 139,628 (0.88\%) SRs fall into the
\emph{large-duration} category (1–2~years). These are concentrated in DPR,
DOHMH, and the Economic Development Corporation (\textsc{EDC}), which together
account for (\SI[round-precision=0]{94}{\percent}).  Additionally, 
1240 SRs were classified as \emph{extreme} positive 
durations ($>$5~years). \SI[round-precision=0]{94}{\percent} 
of these extreme positive durations are  
within DPR.

%-------------------------------------------------------------------------------
% Sub-section: Duplicate & Redundant Fields
%-------------------------------------------------------------------------------

\subsection{Duplicate and Redundant Fields:}
\label{sec:redundant} 
This analysis revealed several redundant fields, indicating a 
potential need for consolidation:

\begin{itemize}[left=1.5em, itemsep=0.25\baselineskip, align=parleft, label=\textbullet]

  \item \texttt{latitude/longitude} \& \texttt{location}:  
  The \texttt{location} field is a pure concatenation of 
  the \texttt{latitude} and \texttt{longitude} fields, with a 
  comma and parentheses added (e.g., 
  \texttt{latitude}: 40.6044676912084, \texttt{longitude}: -73.9667785228311, 
  combine in \texttt{location}: (40.6044676912084, -73.9667785228311)).  
  Arguably, this makes the \texttt{location} field more difficult to use 
  than the individual fields.

  \item \texttt{borough} \& \texttt{park\_borough}:  These are 100\% matches.

  \item \texttt{borough} \& \texttt{taxi\_company\_borough}:  Despite 
  similar names, these fields exhibit only a 0.29\% match 
   indicating that these fields likely serve different purposes. 
  The \texttt{taxi\_company\_borough} field is exclusive to the TLC.

  \item \texttt{agency} \& \texttt{agency\_name}:  
  The \texttt{agency} field contains the abbreviations of City 
  agencies (e.g., NYPD, DOT, DEP), while \texttt{agency\_name} 
  contains the full names (e.g., Department of Environmental Protection).  
  Including both fields seems redundant, especially given that NYC agency 
  abbreviations are widely recognized and understood.

  \item \texttt{landmark} \& \texttt{street\_name}:  
  Per the 311 SR Data Dictionary, the \texttt{landmark} field is intended 
  to refer to noteworthy locations such as parks, hospitals, airports, 
  sports facilities, or performance spaces. 
  However, most entries are actually street names, with a 59.7\% match 
  to the \texttt{street\_name} field. 
  Many non-matches also appear to be logical matches differing only 
  in spelling or formatting (e.g., ``NINTH AVE'' vs.\ ``9 AVE''), 
  suggesting misuse of the \texttt{landmark} field and highlighting redundancy.

  \item \texttt{cross\_street\_1 \& 2} \& \texttt{intersection\_street\_1 \& 2}:  
  These two street pairs are used to help identify the 
  \texttt{incident address}.  
  We found 84\% of these street pairs (1 \& 2) to be duplicates.  
  Given this high level of duplication, it raises the question of which 
  field-pair should be trusted and whether they serve distinct purposes.  
  It is possible that these originated from different agencies using 
  different variable names to represent the same information, but no 
  documentation exists to clarify this relationship.

\end{itemize}

\paragraph{Reducing File Size:}
\label{par:filesize}
Beyond analytical concerns, removing duplicate and near-duplicate fields 
can reduce file size by 18.5\%. A smaller file size 
means faster downloads, reduced storage, and often simpler data analysis. 
Below is a list of duplicate and near-duplicate fields which 
we believe could be removed from the dataset with minimal data loss. 

\begin{itemize}[left=1.5em,itemsep=0.1\baselineskip]
  \item \texttt{agency\_name}: redundant with \texttt{agency}.
  \item \texttt{park\_borough}: identical to \texttt{borough}.
  \item \texttt{location}: a concatenation of \texttt{latitude} and \texttt{longitude} coordinates.
  \item \texttt{cross\_street\_1/2} and \texttt{intersection\_street\_1/2}:
 83\% match; retaining a single pair.
\end{itemize}

Additionally, certain fields may not be useful for broad analysis
due to their sparse data population. This applies to the fields 
below which are $<$0.01\% complete.

\begin{multicols}{2}
\begin{itemize}[left=1.5em]
    \item \texttt{taxi\_company\_borough}
    \item \texttt{taxi\_pick\_up\_location}
    \item \texttt{vehicle\_type}
    \item \texttt{facility\_type}
    \item \texttt{due\_date}
    \item \texttt{bridge\_highway\_direction}
    \item \texttt{bridge\_highway\_segment}
    \item \texttt{road\_ramp}
\end{itemize}
\end{multicols}

It's worth noting that while such sparsity may limit their use in 
broad analyses, these fields can still contain valuable insights 
for specific agency-level studies, given that even a 1\% 
non-blank rate can be meaningful given a 16 million-row 
dataset. Proper handling of sparse fields, 
such as storing them in separate tables or using them in 
targeted analyses, could improve data accessibility and 
efficiency without compromising the dataset’s broader utility.

\paragraph{Field Encoding:} Further size reduction is possible 
by encoding selected categorical variables 
Using \texttt{complaint\_type} as an example,
instead of repeatedly storing the full text of each possible value,
one could use a numeric representation.  For example, rather than
storing verbose strings such as the \texttt{complaint\_type} 
“\texttt{REQUEST LARGE BULKY ITEM COLLECTION}” 632,148 times, 
numeric or symbolic codes could be used instead. Encoding 
not only reduces file size, but would likely improve 
processing efficiency by using numeric values instead of 
text strings. Moreover, standardized categorical inputs 
often prevents misspellings  and invalid entries, ensuring 
cleaner, more reliable data. This approach is particularly 
useful in large datasets, where frequently repeated text-based 
categorical variables significantly increase storage requirements.

Although publishing a flat CSV file supports lightweight 
analysis, it becomes inefficient for analytics 
involving millions of records. One promising alternative to reducing data storage  
would be employment of the cross-platform Apache Arrow package. 
Apache Arrow (arrow.apache.org) is an innovative, cross-language 
development platform designed for in-memory data. 
It defines a standard for representing columnar data, 
enabling efficient real-time analytics and data processing. As such,
it offers dramatic improvements in file storage size, as well as 
efficient analysis and cross-platform support for R, Python, 
and Julia programming languages. Remarkable storage 
reduction (by several orders of magnitude) 
is possible \citep{bates2024csv}.

\subsection{Data Dictionary} 
\label{sec:datadictionary}


The 311 SR Data Dictionary would benefit from an update. We
found several discrepancies between the Data Dictionary and the actual
data, which could potentially mislead analytic efforts. Here are 
some examples.

\paragraph{Data typing:} As previously noted, while the NYC Open Data 
portal displays the data types of various fields, the 
Data Dictionary does not. For example, in the Data Dictionary, 
the \texttt{incident\_zip} field is specified as text. This may 
not be the most suitable data definition. Perhaps a better 
dictionary description would be ``categorical'' with a note 
indicating that the data must contain only numeric values 
and is not subjected to arithmetic operations.

Additionally, the Data Dictionary often shows incomplete or out-of-date
domains of legal values. For example, the \texttt{status} field is noted as 
having values of \textit{assigned, canceled, 
closed}, and \textit{pending}. However, we observed additional values 
such as \textit{in progress, started}, \textit{canceled}, 
and \textit{unspecified}. Similarly, the \texttt{address\_type} field 
show acceptable values of \textit{address, blockface, intersection, latlong}, 
and \textit{placename}. However, we found additional values such 
as \textit{bbl} and \textit{unrecognized}. Other fields, such 
as \texttt{facility\_type}, \texttt{vehicle\_type}, \texttt{taxi\_pick\_up\_location},
\texttt{road\_ramp}, and \texttt{city}, exhibit similar inaccuracies 
in their specified domain values.

%-------------------------------------------------------------------------------
% Section: Principles
%-------------------------------------------------------------------------------

\section{Recommendations for Government Open Data Curation}
\label{sec:recommendations}

Building upon  insights from our case study of the 
NYC 311 SR data, we propose a set of data 
curation principles tailored for government open datasets. 
These principles are designed to address the unique challenges 
 observed in the curation of such datasets, 
ensuring they remain reliable, consistent, and useful for 
public services, research, and other applications.
To facilitate implementation, the principles are prioritized based on
their impact and feasibility, offering a clear roadmap for improving
data curation practices.

\subsection{Principles}
\label{subsec:principles}

\paragraph{Principle 1: Establishing Cross-Organizational Consistency:}
A key challenge in government-released datasets is the 
harmonization of data fields across various agencies, and 
maintaining that consistency over time. Inconsistent field
definitions, data formats, or value domains can lead to erroneous
analyses and hinder cross-agency collaboration.
To address this, agencies should adopt 
standardized naming conventions, value domains, usages, and 
formats across all datasets. Regular audits and data harmonization 
processes should be implemented to ensure consistency 
across long-term datasets, particularly as agency structures 
and reporting standards evolve. For instance, ensuring that 
historical datasets reflect the same field naming conventions 
over a span of years can prevent inconsistent analyses. 
These standards should be promulgated by the Open Data 
governing authority, following agreements by the relevant parties.

\paragraph{Principle 2: Ensuring Data Entry Accuracy:}
Data accuracy and validity are fundamental to the utility of 
open datasets. In the NYC 311 data, issues such as 
invalid ZIP codes and nonsensical date entries 
highlight the need for rigorous validation processes. Governments 
should establish clear protocols for data entry, ensuring that 
fields are populated with valid, legally acceptable values. Accurate
data entry practices reduce the risk of introducing systemic errors,
particularly in high-volume datasets. To
the maximum extent possible, the software should assist in 
ensuring that data entries are accurate and valid. Some examples include

\begin{itemize}[left=1.5em,itemsep=0.15\baselineskip]
    \item Certain date fields should be populated by the application software
     versus manual entry. Dates should be populated
    based upon an action in the system, such as a change in status. 
       
    \item Select fields should be subjected to automated checks 
    that prevent logical errors, such as a closing date occurring before the creation date. 
    
    \item A number of fields have clearly defined domains of legal
    values. These fields should be validated against a reference 
    dataset upon entry. 
 \end{itemize}

\paragraph{Principle 3: Optimizing Storage Efficiency:}
Efficient storage and data representation are crucial for handling 
large government datasets. Our research found several duplicate and
near-duplicate fields in the 311 SR dataset. Removing these
redundancies could reduce the dataset size by 18.5\%, improving
performance without compromising analytical value. Eliminating 
redundant fields and encoding categorical variables can 
significantly reduce file sizes and improve the efficiency 
of data analysis. Similarly, categorical variables could be 
encoded in a standardized format to optimize storage and 
streamline analyses. Reducing file size not only saves 
storage space, but also enhances the performance of queries, 
downloads, and analysis processes.


\paragraph{Principle 4: Maintain and Update Data Dictionaries:}
Clear, accurate, and up-to-date documentation 
is essential for the effective use 
of government datasets. Our study found several
instances where the Data Dictionary did not accurately reflect the 
actual data. Allowable values should be clearly 
referenced in the Data Dictionary, and regular reviews and updates should 
occur especially when there are changes in data structure, usage, or 
context.

\paragraph{Principle 5: Automate Ongoing Quality Assurance Processes:}
Automated data validation and quality assurance processes can 
significantly improve the reliability of government 
datasets. Unusual trends can be identified and irregularities highlighted. 
In the NYC 311 case, issues such as large spikes in SR 
creation and closure at exactly midnight and noon likely 
stem from an automated script that distorts the 
accurate capture of SR durations. Real-time validation systems could
help prevent such errors, ensuring data integrity at the point of 
entry ensuring the data remains clean and reliable 
as it is continuously updated.

\paragraph{Principle 6: Ensuring Data Transparency:}
Transparency is a core tenet of open data, and governments must ensure 
that datasets are easily accessible and 
readily understandable. This includes publishing clear 
metadata, which enables users to understand the 
dataset’s structure, limitations, and intended use
cases. Additionally, governments should communicate transparently
about data quality issues, such as notifying the public when errors in SR 
fields (e.g., erroneous \texttt{closed\_date(s)} are discovered, including
an explanation about how such issues will be resolved.

\subsection{Actionable Steps}
\label{subsec:actions}

The principles outlined above provide a conceptual 
foundation. However, governments need practicable steps to 
operationalize them. Clear priorities help agencies focus 
efforts where they will have the greatest impact. The 
following actions represent a 
structured path toward improving data curation, validation, 
and long-term dataset stewardship.

\paragraph{Real-Time Validation Tools:}
\label{par:realtime}
Governments can begin by investing in real-time validation systems, 
regularly updated data dictionaries, and tools for encoding and 
standardizing data fields. Automated rule-based checks can 
significantly improve data quality at the point of 
data entry.  Automated systems can proactively detect and possibly 
correct common issues, reducing the downstream manual workload.


\paragraph{Collaboration Among Stakeholders:}
\label{par:collaboration}
Effective data curation relies on collaboration among city 
agencies and engagement with the broader open-data community. Regular 
discussions regarding data standards, formats, and metadata ensure that 
datasets support the needs of policymakers, researchers, and 
the public. Clear communication channels help surface recurring 
issues, clarify user requirements, and guide improvements to dataset 
documentation and structure.

\paragraph{Automated Quality-Assurance:}
\label{para:qa}
Implementation of automated QA systems could monitor 
adherence to established validation standards.  Such tools 
could capture and display quality metrics of the data sets as presented
on appropriate dashboards. These dashboards could provide 
a real-time snapshot of data quality, making deviations 
immediately visible and actionable. Deviations in quality metrics 
would be highlighted, prompting appropriate corrective actions

\paragraph{Expert Engagement:}
\label{para:experts}

Engagement with data scientists and statisticians is key to improving 
data quality. Experts can help design curation processes, 
such as developing algorithms for automatic data 
cleaning, identifying inconsistent/invalid data entries, and ensuring curation
rules are maintained over time. Their involvement also
helps to establish best practices for validating data and
maintaining consistency across data sources. 


\paragraph{Robust Feedback Channels:}
\label{par:feedback}
Ongoing feedback from dataset users—including researchers, civic groups, 
and the public—is essential for identifying issues 
and refining data practices. Establishing such feedback loops 
for public engagement ensures that recommendations 
for improving data curation and validation 
are not only heard and discussed, but implemented. 


\paragraph{Public Engagement Through Community Initiatives:}
\label{par:engagement}
Events such as NYC Open Data Week provide a valuable platform for 
collaboration, learning, and innovation. Our involvement in this 
project stemmed from such an event, where we were 
inspired by the potential of open data to enhance
 public services. Government's support of such 
 initiatives, not only promotes data literacy, but 
 encourages community-driven improvements to
datasets, strengthens public engagement and
demonstrate the societal value of open data. 


%-------------------------------------------------------------------------------
%	Section: Conclusion
%-------------------------------------------------------------------------------

\section{Conclusion} 
\label{sec:conclusion}
Our study highlights the critical role of robust data curation in
ensuring the reliability and utility of open datasets, as demonstrated
through the examination of the NYC 311 SR data. As these datasets are
applied across diverse fields such as public services and academic
research, inconsistencies, missing values, and formatting errors can
significantly undermine the insights derived from them. Trustworthy
machine learning systems, which rely heavily on high-quality data, are
especially vulnerable to such issues. Errors in the underlying data,
such as biases or inconsistencies, can compromise both the accuracy
and fairness of predictive models, impacting real-world decisions in
urban planning and policy \citep{rahm2000data, geiger2020garbage}.

While the findings here are specific to NYC 311 data, the principles 
proposed have broader applicability. Datasets in healthcare, 
transportation, and environmental monitoring can similarly benefit 
from these curation strategies, enabling improved outcomes across 
domains. This research further emphasizes the 
necessity of harmonizing data across city agencies, 
particularly resolving discrepancies in field
names, formats, usage, and definitions. Consistency in data is also 
essential for trustworthy machine learning, where 
inconsistencies across sources can
lead to distorted outcomes. This challenge is even more pronounced in
datasets spanning long time periods, as changes in agency structures
or reporting standards and data usage can introduce 
subtle biases and data distortions. Long-term data 
consistency is crucial for longitudinal studies and 
predictive modeling, as even minor 
data shifts can lead to significant deviations in model outcomes, 
as noted by \citet{rahm2000data} and \citet{borgman2012conundrum}.

Finally, the intersection of data curation and machine learning for
public policy applications opens new avenues for improving governance.
High-quality data enables machine learning models to better predict
trends, allocate resources, and address service delivery issues, such
as delays in responding to 311 complaints. The COVID-19 pandemic
underscored the importance of real-time, trustworthy data in crisis
management, where the accuracy and timeliness of data-driven insights
were critical for public health responses. Without proper data
curation, machine learning models used during the pandemic would have
been compromised, affecting decisions on resource allocation and
service provision \citep{worby2020face, khemasuwan2021applications}.
Future efforts could focus on automating data curation processes,
particularly in real-time data pipelines, to ensure that the data used
in machine learning models remains accurate, clean, and reliable
\citep{chu2016data, hurbean2021open}.


%-------------------------------------------------------------------------------
%	Section: Supplementary Material
%-------------------------------------------------------------------------------

\section*{Supplementary Material}
The \textsc{R} source code used to produce these results is available at:
\url{https://github.com/tusseyd/journal_of_data_science/tree/main}.  

The USPS Zip Code reference file is available at: 
\url{https://www.unitedstateszipcodes.org/zip-code-database/}.

The 311 SR dataset used for this analysis is available at: \url{https://figshare.com/articles/dataset/_/30699800}



Tussey, David (2025). Data set in ZIP format for the 311_nyc_data_cleaning project for the Journal of Data Science. figshare. Dataset. https://figshare.com/articles/dataset/_/30699800

\bibliographystyle{jds}
\bibliography{refs}

\end{document}